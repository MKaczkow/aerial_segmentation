{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-01T19:06:50.703989Z","iopub.status.busy":"2024-05-01T19:06:50.703164Z","iopub.status.idle":"2024-05-01T19:06:50.779370Z","shell.execute_reply":"2024-05-01T19:06:50.778360Z","shell.execute_reply.started":"2024-05-01T19:06:50.703950Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Limited data"]},{"cell_type":"markdown","metadata":{},"source":["### Mask are RGB but give the HEX color\n","\n","### convert + onehot coding,we need softmax\n","\n","### Importantly, don't resize in semantic segmentation just crop "]},{"cell_type":"markdown","metadata":{},"source":["#### 1. Patchify"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:06:50.781420Z","iopub.status.busy":"2024-05-01T19:06:50.781108Z","iopub.status.idle":"2024-05-01T19:07:03.356163Z","shell.execute_reply":"2024-05-01T19:07:03.354878Z","shell.execute_reply.started":"2024-05-01T19:06:50.781395Z"},"trusted":true},"outputs":[],"source":["# !pip install patchify"]},{"cell_type":"markdown","metadata":{},"source":["1. Tile 1 797*644 -> 768*512 -> 6\n","2. Tile 2 509* 544 -> 512*256 ->2\n","3. Tile 3 682*658 -> 512*512 ->4\n","4. Tile 4 1099*846 --> 1024*768 -->12\n","5. Tile 5 1126*1058 --> 1024*1024 -->16\n","6. Tile 6 859*838 --> 768*768 -->9\n","7. Tile 7 1817*2061 -> 1972*2048 -> 50\n","8. Tile 8 2149*1479 -> 2048*1280 -> 40"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:03.358141Z","iopub.status.busy":"2024-05-01T19:07:03.357803Z","iopub.status.idle":"2024-05-01T19:07:03.364274Z","shell.execute_reply":"2024-05-01T19:07:03.363239Z","shell.execute_reply.started":"2024-05-01T19:07:03.358111Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from patchify import patchify\n","from PIL import Image\n","import torch\n","from sklearn.preprocessing import MinMaxScaler,StandardScaler"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:03.365766Z","iopub.status.busy":"2024-05-01T19:07:03.365507Z","iopub.status.idle":"2024-05-01T19:07:03.377465Z","shell.execute_reply":"2024-05-01T19:07:03.376635Z","shell.execute_reply.started":"2024-05-01T19:07:03.365744Z"},"trusted":true},"outputs":[],"source":["def create_dir(dir_name):\n","    if not os.path.exists(dir_name):\n","        os.makedirs(dir_name, exist_ok=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:03.380820Z","iopub.status.busy":"2024-05-01T19:07:03.380553Z","iopub.status.idle":"2024-05-01T19:07:03.389919Z","shell.execute_reply":"2024-05-01T19:07:03.389111Z","shell.execute_reply.started":"2024-05-01T19:07:03.380798Z"},"trusted":true},"outputs":[],"source":["scaler = MinMaxScaler()\n","root_dir = 'data\\DubaiSemanticSegmentationDataset'\n","patch_size = 256"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:03.391696Z","iopub.status.busy":"2024-05-01T19:07:03.391330Z","iopub.status.idle":"2024-05-01T19:07:16.061712Z","shell.execute_reply":"2024-05-01T19:07:16.060905Z","shell.execute_reply.started":"2024-05-01T19:07:03.391663Z"},"trusted":true},"outputs":[],"source":["image_dataset=[]\n","for path,subdirs,files in os.walk(root_dir):\n","    dirname = path.split(os.path.sep)[-1]\n","    if dirname ==\"images\":\n","        images = os.listdir(path)\n","        for i,image_name in enumerate(images):\n","            if image_name.endswith(\".jpg\"):\n","                image = cv2.imread(path+'/'+image_name,cv2.IMREAD_COLOR)\n","                # Crop\n","                # Height Width Channel\n","                SIZE_X = (image.shape[1]//patch_size)*patch_size \n","                SIZE_Y = (image.shape[0]//patch_size)*patch_size\n","                image = Image.fromarray(image)\n","                image = image.crop((0,0,SIZE_X,SIZE_Y))\n","                \n","                image = np.array(image)\n","                \n","                patches_img = patchify(image,(patch_size,patch_size,3),step=patch_size)\n","                for i in range(patches_img.shape[0]):\n","                    for j in range(patches_img.shape[1]):\n","                        single_patch_img = patches_img[i,j,:,:]\n","                        # img need flatten\n","                        single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n","                        single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.\n","                        image_dataset.append(single_patch_img)\n","                \n","                "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:16.063079Z","iopub.status.busy":"2024-05-01T19:07:16.062806Z","iopub.status.idle":"2024-05-01T19:07:17.743835Z","shell.execute_reply":"2024-05-01T19:07:17.742761Z","shell.execute_reply.started":"2024-05-01T19:07:16.063055Z"},"trusted":true},"outputs":[],"source":["mask_dataset =[]\n","for path,subdirs,files in os.walk(root_dir):\n","    dirname = path.split(os.path.sep)[-1]\n","    if dirname == \"masks\":\n","        masks = os.listdir(path)\n","        for i,mask_name in enumerate(masks):\n","            if mask_name.endswith(\".png\"):\n","                mask = cv2.imread(path+'/'+mask_name,1)\n","                mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n","                SIZE_X = (mask.shape[1]//patch_size)*patch_size\n","                SIZE_Y = (mask.shape[0]//patch_size)*patch_size\n","                mask = Image.fromarray(mask)\n","                mask = mask.crop((0,0,SIZE_X,SIZE_Y))\n","                mask = np.array(mask)\n","                \n","                patches_mask = patchify(mask,(patch_size,patch_size,3),step = patch_size)\n","                \n","                for i in range(patches_mask.shape[0]):\n","                    for j in range(patches_mask.shape[1]):\n","                        single_patch_mask = patches_mask[i,j,:,:]\n","                        single_patch_mask = single_patch_mask[0]\n","                        mask_dataset.append(single_patch_mask)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:17.745575Z","iopub.status.busy":"2024-05-01T19:07:17.745182Z","iopub.status.idle":"2024-05-01T19:07:18.424980Z","shell.execute_reply":"2024-05-01T19:07:18.423841Z","shell.execute_reply.started":"2024-05-01T19:07:17.745543Z"},"trusted":true},"outputs":[],"source":["image_dataset =np.asarray(image_dataset)\n","mask_dataset =np.asarray(mask_dataset)\n","image_dataset.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Draw"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:18.426781Z","iopub.status.busy":"2024-05-01T19:07:18.426406Z","iopub.status.idle":"2024-05-01T19:07:19.021443Z","shell.execute_reply":"2024-05-01T19:07:19.020317Z","shell.execute_reply.started":"2024-05-01T19:07:18.426747Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","image_number = random.randint(0,len(image_dataset))\n","plt.figure(figsize=(12,6))\n","plt.subplot(121)\n","plt.imshow(np.reshape(image_dataset[image_number], (patch_size, patch_size, 3)))\n","plt.subplot(122)\n","plt.imshow(np.reshape(mask_dataset[image_number], (patch_size, patch_size, 3)))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Convert  Mask Color "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:19.023172Z","iopub.status.busy":"2024-05-01T19:07:19.022814Z","iopub.status.idle":"2024-05-01T19:07:19.035943Z","shell.execute_reply":"2024-05-01T19:07:19.034917Z","shell.execute_reply.started":"2024-05-01T19:07:19.023139Z"},"trusted":true},"outputs":[],"source":["Building = '#3C1098'.lstrip('#')\n","Building = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n","\n","Land = '#8429F6'.lstrip('#')\n","Land = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n","\n","Road = '#6EC1E4'.lstrip('#') \n","Road = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n","\n","Vegetation =  'FEDD3A'.lstrip('#') \n","Vegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n","\n","Water = 'E2A929'.lstrip('#') \n","Water = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n","\n","Unlabeled = '#9B9B9B'.lstrip('#') \n","Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:19.039217Z","iopub.status.busy":"2024-05-01T19:07:19.038897Z","iopub.status.idle":"2024-05-01T19:07:19.054921Z","shell.execute_reply":"2024-05-01T19:07:19.053960Z","shell.execute_reply.started":"2024-05-01T19:07:19.039170Z"},"trusted":true},"outputs":[],"source":["def rgb_to_2D_label(label,num_class=6):\n","    \"\"\"\n","    Suply our labale masks as input in RGB format. \n","    Replace pixels with specific RGB values ...\n","    \"\"\"\n","    label_seg = np.zeros(label.shape,dtype=np.uint8)\n","    label_seg [np.all(label == Building,axis=-1)] = 0\n","    label_seg [np.all(label==Land,axis=-1)] = 1\n","    label_seg [np.all(label==Road,axis=-1)] = 2\n","    label_seg [np.all(label==Vegetation,axis=-1)] = 3\n","    label_seg [np.all(label==Water,axis=-1)] = 4\n","    label_seg [np.all(label==Unlabeled,axis=-1)] = 5\n","    \n","    label_seg = label_seg[:,:,0]  #Just take the first channel, no need for all 3 channels\n","    \n","    \n","    new_label = np.zeros(label_seg.shape + (num_class,))\n","    \n","    \n","    #  将平面的label的每类，都单独变成一层\n","    for i in range(num_class):\n","        new_label[label_seg == i,i] = 1   \n","    label_seg=new_label\n","    \n","    return label_seg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:19.056740Z","iopub.status.busy":"2024-05-01T19:07:19.056373Z","iopub.status.idle":"2024-05-01T19:07:45.678812Z","shell.execute_reply":"2024-05-01T19:07:45.677739Z","shell.execute_reply.started":"2024-05-01T19:07:19.056709Z"},"trusted":true},"outputs":[],"source":["labels = []\n","for i in range(mask_dataset.shape[0]):\n","    label = rgb_to_2D_label(mask_dataset[i])\n","    labels.append(label)\n","    \n","\n","labels = np.array(labels)\n","\n","# labels=np.expand_dims(labels,axis=3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:45.682323Z","iopub.status.busy":"2024-05-01T19:07:45.681633Z","iopub.status.idle":"2024-05-01T19:07:45.689298Z","shell.execute_reply":"2024-05-01T19:07:45.688161Z","shell.execute_reply.started":"2024-05-01T19:07:45.682284Z"},"trusted":true},"outputs":[],"source":["image_dataset[0].shape,labels.shape"]},{"cell_type":"markdown","metadata":{},"source":["### Checking again "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:45.694041Z","iopub.status.busy":"2024-05-01T19:07:45.693705Z","iopub.status.idle":"2024-05-01T19:07:46.229457Z","shell.execute_reply":"2024-05-01T19:07:46.228516Z","shell.execute_reply.started":"2024-05-01T19:07:45.694016Z"},"trusted":true},"outputs":[],"source":["image_number = random.randint(0,len(image_dataset))\n","plt.figure(figsize=(12,6))\n","plt.subplot(121)\n","plt.imshow(np.reshape(image_dataset[image_number], (patch_size, patch_size, 3)))\n","plt.subplot(122)\n","plt.imshow(np.reshape(mask_dataset[image_number], (patch_size, patch_size, 3)))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:46.231216Z","iopub.status.busy":"2024-05-01T19:07:46.230850Z","iopub.status.idle":"2024-05-01T19:07:46.241589Z","shell.execute_reply":"2024-05-01T19:07:46.240647Z","shell.execute_reply.started":"2024-05-01T19:07:46.231172Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","from torch.utils.data import DataLoader,Dataset\n","class Aerial(Dataset):\n","    def __init__(self,images,maskes):\n","        super(Aerial,self).__init__()\n","        self.images = images\n","        self.masks = maskes\n","        self.n_samples =len(images) \n","        \n","    \n","    def __getitem__(self,index):\n","        image = self.images[index]\n","        image = image/255.0\n","        image = np.transpose(image,(2,0,1))\n","        image = image.astype(np.float32)\n","        image = torch.from_numpy(image)\n","        \n","        mask = self.masks[index]\n","        mask = np.transpose(mask,(2,0,1))\n","        mask = mask.astype(np.float32)\n","        mask = torch.from_numpy(mask)\n","        \n","        return image,mask\n","    def __len__(self):\n","        return self.n_samples\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:07:46.243069Z","iopub.status.busy":"2024-05-01T19:07:46.242796Z","iopub.status.idle":"2024-05-01T19:08:27.860304Z","shell.execute_reply":"2024-05-01T19:08:27.859491Z","shell.execute_reply.started":"2024-05-01T19:07:46.243038Z"},"trusted":true},"outputs":[],"source":["create_dir(\"files/\")\n","create_dir(\"Results/\")\n","H = 256\n","W =256\n","num_class = len(np.unique(labels))\n","size =(H,W)\n","batch_size =2\n","num_epochs = 15\n","lr=1e-4\n","checkpoints_path =\"files/checkpoints.pth\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:27.861860Z","iopub.status.busy":"2024-05-01T19:08:27.861506Z","iopub.status.idle":"2024-05-01T19:08:29.481478Z","shell.execute_reply":"2024-05-01T19:08:29.480155Z","shell.execute_reply.started":"2024-05-01T19:08:27.861827Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_x,val_x,train_y,val_y = train_test_split(image_dataset,labels,test_size=0.2, random_state = 42)\n","\n","train_dataset = Aerial(train_x,train_y)\n","val_dataset = Aerial(val_x,val_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.483283Z","iopub.status.busy":"2024-05-01T19:08:29.482919Z","iopub.status.idle":"2024-05-01T19:08:29.489462Z","shell.execute_reply":"2024-05-01T19:08:29.488530Z","shell.execute_reply.started":"2024-05-01T19:08:29.483254Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle = True,\n","    num_workers=2,\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=2\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.491235Z","iopub.status.busy":"2024-05-01T19:08:29.490903Z","iopub.status.idle":"2024-05-01T19:08:29.509358Z","shell.execute_reply":"2024-05-01T19:08:29.508525Z","shell.execute_reply.started":"2024-05-01T19:08:29.491185Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","import torchvision\n","\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1),  # in_ch、out_ch是通道数\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super(UNet, self).__init__()\n","        self.conv1 = DoubleConv(in_ch, 64)\n","        self.pool1 = nn.MaxPool2d(2)  # 每次把图像尺寸缩小一半\n","        self.conv2 = DoubleConv(64, 128)\n","        self.pool2 = nn.MaxPool2d(2)\n","        self.conv3 = DoubleConv(128, 256)\n","        self.pool3 = nn.MaxPool2d(2)\n","        self.conv4 = DoubleConv(256, 512)\n","        self.pool4 = nn.MaxPool2d(2)\n","        self.conv5 = DoubleConv(512, 1024)\n","        # 逆卷积\n","        self.up6 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n","        self.conv6 = DoubleConv(1024, 512)\n","        self.up7 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n","        self.conv7 = DoubleConv(512, 256)\n","        self.up8 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n","        self.conv8 = DoubleConv(256, 128)\n","        self.up9 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n","        self.conv9 = DoubleConv(128, 64)\n","\n","        self.conv10 = nn.Conv2d(64, out_ch, 1)\n","        self.sigmoid = nn.Sigmoid()\n","    def forward(self, x):\n","        c1 = self.conv1(x)\n","        p1 = self.pool1(c1)\n","        c2 = self.conv2(p1)\n","        p2 = self.pool2(c2)\n","        c3 = self.conv3(p2)\n","        p3 = self.pool3(c3)\n","        c4 = self.conv4(p3)\n","        p4 = self.pool4(c4)\n","        c5 = self.conv5(p4)\n","        up_6 = self.up6(c5)\n","        merge6 = torch.cat([up_6, c4], dim=1)  # 按维数1（列）拼接,列增加\n","        c6 = self.conv6(merge6)\n","        up_7 = self.up7(c6)\n","        merge7 = torch.cat([up_7, c3], dim=1)\n","        c7 = self.conv7(merge7)\n","        up_8 = self.up8(c7)\n","        merge8 = torch.cat([up_8, c2], dim=1)\n","        c8 = self.conv8(merge8)\n","        up_9 = self.up9(c8)\n","        merge9 = torch.cat([up_9, c1], dim=1)\n","        c9 = self.conv9(merge9)\n","        c10 = self.conv10(c9)\n","\n","        out = self.sigmoid(c10)\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.510775Z","iopub.status.busy":"2024-05-01T19:08:29.510500Z","iopub.status.idle":"2024-05-01T19:08:29.527053Z","shell.execute_reply":"2024-05-01T19:08:29.526317Z","shell.execute_reply.started":"2024-05-01T19:08:29.510752Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class DiceLoss(nn.Module):\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, input, target):\n","        smooth = 1.0\n","\n","        iflat = input.reshape(-1)\n","        tflat = target.reshape(-1)\n","        intersection = (iflat * tflat).sum()\n","\n","        dice_loss = 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n","\n","        return dice_loss\n","\n","    def calculate_average_dice_loss(self, inputs, targets):\n","        num_channels = inputs.size(1)\n","\n","        # Calculate Dice loss for each channel\n","        dice_losses = []\n","        for channel in range(num_channels):\n","            input_channel = inputs[:, channel, ...].unsqueeze(1)\n","            target_channel = targets[:, channel, ...].unsqueeze(1)\n","            dice_loss_channel = self.forward(input_channel, target_channel)\n","            dice_losses.append(dice_loss_channel)\n","\n","        # Calculate average Dice loss\n","        average_dice_loss = torch.mean(torch.stack(dice_losses))\n","\n","        return average_dice_loss\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.529086Z","iopub.status.busy":"2024-05-01T19:08:29.528334Z","iopub.status.idle":"2024-05-01T19:08:29.826074Z","shell.execute_reply":"2024-05-01T19:08:29.825315Z","shell.execute_reply.started":"2024-05-01T19:08:29.529054Z"},"trusted":true},"outputs":[],"source":["model = UNet(3,6)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n","weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n","loss_fn = DiceLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.827412Z","iopub.status.busy":"2024-05-01T19:08:29.827103Z","iopub.status.idle":"2024-05-01T19:08:29.834002Z","shell.execute_reply":"2024-05-01T19:08:29.833012Z","shell.execute_reply.started":"2024-05-01T19:08:29.827387Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","def train(model,loader,loss_fn,optimizer,device):\n","    epoch_loss=0.\n","    model.train()\n","    \n","    for x,y in tqdm(loader):\n","        x =x.to(device,dtype = torch.float32)\n","        y = y.to(device,dtype = torch.float32)\n","        optimizer.zero_grad()\n","        outputs = model(x)\n","        loss = loss_fn.calculate_average_dice_loss(outputs, y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss +=loss.item()\n","    \n","    return epoch_loss/len(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.835617Z","iopub.status.busy":"2024-05-01T19:08:29.835050Z","iopub.status.idle":"2024-05-01T19:08:29.851272Z","shell.execute_reply":"2024-05-01T19:08:29.850561Z","shell.execute_reply.started":"2024-05-01T19:08:29.835586Z"},"trusted":true},"outputs":[],"source":["def validate(model,loader,loss_fn,device):\n","    epoch_loss = 0.0\n","    model.eval()\n","    with torch.no_grad():\n","        for x,y in loader:\n","            x =x.to(device,dtype = torch.float32)\n","            y = y.to(device,dtype = torch.float32)\n","            outputs = model(x)\n","            loss = loss_fn.calculate_average_dice_loss(outputs, y)\n","            epoch_loss +=loss.item()\n","            \n","    return epoch_loss/len(loader)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:08:29.852556Z","iopub.status.busy":"2024-05-01T19:08:29.852271Z","iopub.status.idle":"2024-05-01T19:27:13.796544Z","shell.execute_reply":"2024-05-01T19:27:13.795274Z","shell.execute_reply.started":"2024-05-01T19:08:29.852513Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","from IPython.display import clear_output\n","train_losses = []\n","cross_losses = []\n","bestScore=float('inf')\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader,loss_fn, optimizer, device)\n","    Current_loss = validate(model,val_loader,loss_fn,device)\n","    #print(Current_loss)\n","    #print(CurrentScore)\n","    train_losses.append(train_loss)\n","    cross_losses.append(Current_loss)\n","\n","    clear_output(wait=True)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}')\n","    print(f'Cross Acc: {Current_loss:.4f}')\n","    #scheduler.step()\n","\n","    # initialize 3 subplots to plot the loss curve, learning rate curve and accuracy curve\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, label='Training Loss')\n","    plt.plot(cross_losses, label='Validation Loss')\n","    plt.title('Training Loss and Validation Loss')\n","    plt.legend()\n","    plt.show(block=False)\n","    \n","    \n","    if Current_loss < bestScore:\n","        bestScore = Current_loss\n","        best_epoch = epoch\n","        torch.save(model.state_dict(), checkpoints_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:27:13.799180Z","iopub.status.busy":"2024-05-01T19:27:13.798303Z","iopub.status.idle":"2024-05-01T19:27:13.804512Z","shell.execute_reply":"2024-05-01T19:27:13.803525Z","shell.execute_reply.started":"2024-05-01T19:27:13.799136Z"},"trusted":true},"outputs":[],"source":["print(1-bestScore)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:27:13.805960Z","iopub.status.busy":"2024-05-01T19:27:13.805682Z","iopub.status.idle":"2024-05-01T19:27:13.817146Z","shell.execute_reply":"2024-05-01T19:27:13.816437Z","shell.execute_reply.started":"2024-05-01T19:27:13.805936Z"},"trusted":true},"outputs":[],"source":["REAL_img_path = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/masks/image_part_001.png\"\n","img_path = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset/Tile 6/images/image_part_001.jpg\"  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:27:13.818513Z","iopub.status.busy":"2024-05-01T19:27:13.818229Z","iopub.status.idle":"2024-05-01T19:27:20.170025Z","shell.execute_reply":"2024-05-01T19:27:20.168869Z","shell.execute_reply.started":"2024-05-01T19:27:13.818488Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import transforms\n","from PIL import Image\n","import numpy as np\n","from patchify import patchify, unpatchify\n","import matplotlib.pyplot as plt\n","\n","model = UNet(3,6)\n","model.load_state_dict(torch.load(checkpoints_path, map_location=device))\n","\n","\n","transform = transforms.Compose([\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","img = Image.open(img_path)\n","\n","patch_size = 256\n","SIZE_X = (img.size[0] // patch_size) * patch_size\n","SIZE_Y = (img.size[1] // patch_size) * patch_size\n","large_img = img.crop((0, 0, SIZE_X, SIZE_Y))\n","large_img = np.array(large_img)\n","\n","patches_img = patchify(large_img, (patch_size, patch_size, 3), step=patch_size)\n","patches_img = patches_img[:, :, 0, :, :, :]\n","\n","patched_prediction = []\n","for i in range(patches_img.shape[0]):\n","    for j in range(patches_img.shape[1]):\n","        single_patch_img = patches_img[i, j, :, :, :]\n","        single_patch_tensor = torch.from_numpy(single_patch_img).float()\n","        single_patch_tensor /= 255.0\n","        single_patch_tensor = single_patch_tensor.permute(2, 0, 1)\n","\n","        single_patch_tensor = transform(single_patch_tensor).unsqueeze(0)  \n","\n","        with torch.no_grad():\n","            pred = model(single_patch_tensor)\n","        pred = torch.argmax(pred, dim=1).squeeze(0).cpu().numpy()\n","        patched_prediction.append(pred)\n","\n","patched_prediction = np.array(patched_prediction)\n","patched_prediction = np.reshape(patched_prediction, [patches_img.shape[0], patches_img.shape[1], patch_size, patch_size])\n","\n","unpatched_prediction = unpatchify(patched_prediction, (SIZE_Y, SIZE_X))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:27:20.171618Z","iopub.status.busy":"2024-05-01T19:27:20.171325Z","iopub.status.idle":"2024-05-01T19:27:20.184171Z","shell.execute_reply":"2024-05-01T19:27:20.183120Z","shell.execute_reply.started":"2024-05-01T19:27:20.171593Z"},"trusted":true},"outputs":[],"source":["def label_to_rgb(predicted_image):\n","    \n","    Building = '#3C1098'.lstrip('#')\n","    Building = np.array(tuple(int(Building[i:i+2], 16) for i in (0, 2, 4))) # 60, 16, 152\n","    \n","    Land = '#8429F6'.lstrip('#')\n","    Land = np.array(tuple(int(Land[i:i+2], 16) for i in (0, 2, 4))) #132, 41, 246\n","    \n","    Road = '#6EC1E4'.lstrip('#') \n","    Road = np.array(tuple(int(Road[i:i+2], 16) for i in (0, 2, 4))) #110, 193, 228\n","    \n","    Vegetation =  'FEDD3A'.lstrip('#') \n","    Vegetation = np.array(tuple(int(Vegetation[i:i+2], 16) for i in (0, 2, 4))) #254, 221, 58\n","    \n","    Water = 'E2A929'.lstrip('#') \n","    Water = np.array(tuple(int(Water[i:i+2], 16) for i in (0, 2, 4))) #226, 169, 41\n","    \n","    Unlabeled = '#9B9B9B'.lstrip('#') \n","    Unlabeled = np.array(tuple(int(Unlabeled[i:i+2], 16) for i in (0, 2, 4))) #155, 155, 155\n","    \n","    \n","    \n","    segmented_img = np.empty((predicted_image.shape[0], predicted_image.shape[1], 3))\n","    \n","    segmented_img[(predicted_image == 0)] = Building\n","    segmented_img[(predicted_image == 1)] = Land\n","    segmented_img[(predicted_image == 2)] = Road\n","    segmented_img[(predicted_image == 3)] = Vegetation\n","    segmented_img[(predicted_image == 4)] = Water\n","    segmented_img[(predicted_image == 5)] = Unlabeled\n","    \n","    segmented_img = segmented_img.astype(np.uint8)\n","    return(segmented_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T19:27:20.185543Z","iopub.status.busy":"2024-05-01T19:27:20.185265Z","iopub.status.idle":"2024-05-01T19:27:20.919779Z","shell.execute_reply":"2024-05-01T19:27:20.918800Z","shell.execute_reply.started":"2024-05-01T19:27:20.185519Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# Load the images\n","G_img = Image.open(img_path)\n","REAL_img = Image.open(REAL_img_path)\n","\n","# Define patch size and adjust the size of G_img\n","patch_size = 256\n","SIZE_X = (G_img.size[0] // patch_size) * patch_size\n","SIZE_Y = (G_img.size[1] // patch_size) * patch_size\n","G_img = G_img.crop((0, 0, SIZE_X, SIZE_Y))\n","G_img = np.array(G_img)\n","\n","\n","patch_size = 256\n","SIZE_X = (REAL_img.size[0] // patch_size) * patch_size\n","SIZE_Y = (REAL_img.size[1] // patch_size) * patch_size\n","REAL_img = REAL_img.crop((0, 0, SIZE_X, SIZE_Y))\n","REAL_img = np.array(REAL_img)\n","REAL_img = np.array(REAL_img)\n","\n","\n","prediction = label_to_rgb(unpatched_prediction)\n","\n","# Plot the images\n","plt.figure(figsize=(15, 5))\n","plt.subplot(1, 3, 1)\n","plt.imshow(G_img)\n","plt.title(\"The Image\")\n","plt.axis('off')\n","\n","plt.subplot(1, 3, 2)\n","plt.imshow(REAL_img)\n","plt.title(\"Real Mask\")\n","plt.axis('off')\n","\n","plt.subplot(1, 3, 3)\n","plt.imshow(prediction)\n","plt.title(\"Predicted Mask\")\n","plt.axis('off')\n","\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":681625,"sourceId":1196732,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":4}
