{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, Resize\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from src.models.BaselineModel import BaselineModel\n",
    "from src.evaluation.evaluate_result import evaluate_result\n",
    "from src.datasets.DubaiSemanticSegmentationDataset import (\n",
    "    DubaiSemanticSegmentationDataset,\n",
    ")\n",
    "\n",
    "from src.datasets.utils.ResizeToDivisibleBy32 import ResizeToDivisibleBy32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "BATCH_SIZE = 1\n",
    "SEED = 42\n",
    "DUBAI_DATASET_PATH = \"data/DubaiSemanticSegmentationDataset\"\n",
    "IMAGE_SIZE = 576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DubaiSemanticSegmentationDataset(\n",
    "    DUBAI_DATASET_PATH,\n",
    "    transforms=[Compose([Resize(IMAGE_SIZE), ResizeToDivisibleBy32()])],\n",
    ")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 736])\n",
      "torch.Size([1, 3, 576, 736])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=6,  # model output channels (number of classes in your dataset)\n",
    "    activation=\"softmax\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel(classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 640])\n",
      "torch.Size([1, 3, 576, 640])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        # (batch_size, channels, height, width)\n",
    "        # (B, C, H, W)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = torch.argmax(output, dim=1)\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = torch.argmax(output_baseline, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(254, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(masks.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(41, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(masks.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 640])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 640])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1668, 1: 19413, 2: 43, 3: 244335, 4: 2279, 5: 100902}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(output.cpu(), return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{41: 363684, 42: 284, 43: 184, 44: 172, 45: 108, 46: 99, 47: 94, 48: 93, 49: 71, 50: 86, 51: 60, 52: 68, 53: 65, 54: 77, 55: 59, 56: 59, 57: 80, 58: 1055, 59: 80, 60: 67, 61: 41, 62: 59, 63: 68, 64: 46, 65: 40, 66: 37, 67: 46, 68: 48, 69: 36, 70: 37, 71: 45, 72: 27, 73: 30, 74: 37, 75: 32, 76: 30, 77: 22, 78: 25, 79: 30, 80: 34, 81: 35, 82: 35, 83: 30, 84: 37, 85: 25, 86: 33, 87: 48, 88: 29, 89: 30, 90: 39, 91: 29, 92: 15, 93: 19, 94: 29, 95: 22, 96: 20, 97: 31, 98: 29, 99: 34, 100: 37, 101: 28, 102: 24, 103: 28, 104: 42, 105: 31, 106: 24, 107: 28, 108: 23, 109: 25, 110: 33, 111: 33, 112: 34, 113: 29, 114: 23, 115: 20, 116: 36, 117: 25, 118: 24, 119: 31, 120: 25, 121: 22, 122: 27, 123: 25, 124: 36, 125: 26, 126: 33, 127: 22, 128: 33, 129: 33, 130: 22, 131: 25, 132: 64112, 133: 221, 134: 152, 135: 103, 136: 102, 137: 119, 138: 79, 139: 91, 140: 79, 141: 78, 142: 70, 143: 76, 144: 76, 145: 61, 146: 99, 147: 60, 148: 72, 149: 55, 150: 70, 151: 63, 152: 72, 153: 90, 154: 60, 155: 142, 156: 65, 157: 76, 158: 64, 159: 63, 160: 75, 161: 68, 162: 60, 163: 75, 164: 73, 165: 67, 166: 99, 167: 130, 168: 173, 169: 299667, 170: 64, 171: 50, 172: 45, 173: 60, 174: 46, 175: 45, 176: 41, 177: 44, 178: 70, 179: 36, 180: 24, 181: 44, 182: 35, 183: 50, 184: 43, 185: 41, 186: 25, 187: 39, 188: 41, 189: 42, 190: 37, 191: 34, 192: 35, 193: 44, 194: 42, 195: 50, 196: 44, 197: 44, 198: 43, 199: 38, 200: 48, 201: 37, 202: 36, 203: 60, 204: 56, 205: 49, 206: 56, 207: 49, 208: 41, 209: 51, 210: 51, 211: 57, 212: 67, 213: 54, 214: 65, 215: 64, 216: 75, 217: 83, 218: 84, 219: 114, 220: 134, 221: 1032, 222: 65, 223: 93, 224: 113, 225: 169, 226: 299703, 227: 50, 228: 51, 229: 39, 230: 31, 231: 53, 232: 38, 233: 40, 234: 32, 235: 53, 236: 54, 237: 55, 238: 42, 239: 60, 240: 64, 241: 55, 242: 80, 243: 81, 244: 112, 245: 140, 246: 64048, 247: 25, 248: 16, 249: 24, 250: 15, 251: 30, 252: 33, 253: 67, 254: 982}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(masks.cpu(), return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions should match, but ``output`` shape is not equal to ``target`` shape, torch.Size([576, 640]) != torch.Size([3, 576, 640])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# FIXME: wykonanie tego bez błędu == tensory mają takie same wymiary\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mevaluate_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulticlass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\src\\evaluation\\evaluate_result.py:37\u001b[0m, in \u001b[0;36mevaluate_result\u001b[1;34m(pred, target, mode, num_classes)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported. Choose either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m     tp, fp, fn, tn \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     tp, fp, fn, tn \u001b[38;5;241m=\u001b[39m smp\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mget_stats(\n\u001b[0;32m     45\u001b[0m         pred\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mint(), target\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mint(), mode\u001b[38;5;241m=\u001b[39mmode\n\u001b[0;32m     46\u001b[0m     )\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\segmentation_models_pytorch\\metrics\\functional.py:133\u001b[0m, in \u001b[0;36mget_stats\u001b[1;34m(output, target, mode, ignore_index, threshold, num_classes)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``threshold`` parameter does not supported for this \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions should match, but ``output`` shape is not equal to ``target`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``ignore_index`` parameter is not supproted for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions should match, but ``output`` shape is not equal to ``target`` shape, torch.Size([576, 640]) != torch.Size([3, 576, 640])"
     ]
    }
   ],
   "source": [
    "# FIXME: wykonanie tego bez błędu == tensory mają takie same wymiary\n",
    "evaluate_result(output, masks, mode=\"multiclass\", num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_result(output_baseline, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "metrics_dict = {\n",
    "    \"iou\": [],\n",
    "    \"f1\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        output = model(images.to(device))\n",
    "        # output = (output > 0.5).float()\n",
    "\n",
    "        iter_metrics = evaluate_result(output, masks)\n",
    "        for key in metrics_dict.keys():\n",
    "            metrics_dict[key].append(iter_metrics[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
