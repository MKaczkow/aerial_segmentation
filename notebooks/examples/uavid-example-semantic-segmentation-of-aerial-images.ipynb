{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation of High-Resolution Aerial Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone!\n",
    "\n",
    "\n",
    "\n",
    "![](https://uavid.nl/UAVid_files/imgs/UAVid_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will work with [UAVid](https://uavid.nl/#home) dataset, which focusing on urban scenes. Our goal is to predict per-pixel semantic labeling.\n",
    "\n",
    "And the evaluation metric is IoU. \n",
    "![](https://www.oreilly.com/library/view/deep-learning-for/9781788295628/assets/63fb2c41-8e83-49c5-ad3a-fee59e8a178b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we’ll use the following notable libraries: \n",
    "\n",
    "* [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) has a lot of encoders for each model architecture.\n",
    "* [albumentations](https://github.com/albumentations-team/albumentations) has spatial-level transforms that change both an input image and mask simultaneously.\n",
    "* [catalyst](https://github.com/catalyst-team/catalyst) PyTorch framework that helps with reproducibility, fast experimentation and has a lot of useful utils.\n",
    "* [wandb](https://www.wandb.com/) Logger to track metrics, save hyper-parameters, gradients and model checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we’ll install `segmentation-models-pytorch` and `torch nightly` for native amp support. And import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch\n",
    "# !pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"True\"\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from catalyst import utils\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, List, Tuple\n",
    "import torch\n",
    "import catalyst\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def example(\n",
    "    image_path=r\"data\\UAVidSemanticSegmentationDataset/train/train/seq33/Images/000200.png\",\n",
    "    mask_path=r\"data\\UAVidSemanticSegmentationDataset/train/train/seq33/Labels/000200.png\",\n",
    "):\n",
    "    image = Image.open(image_path)\n",
    "    mask = Image.open(mask_path)\n",
    "    plt.figure(figsize=(18, 24))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask)\n",
    "\n",
    "\n",
    "example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset comprises 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes.\n",
    "The image resolution is mostly 3840×2160 or 4096×2160.\n",
    "\n",
    "![](https://i.ibb.co/r73CD9M/colors.png)\n",
    "\n",
    "1. building: living houses, garages, skyscrapers, security booths, and buildings under construction.\n",
    "2. road: road or bridge surface that cars can run on legally. Parking lots are not included.\n",
    "3. tree: tall trees that have canopies and main trunks.\n",
    "4. low vegetation: grass, bushes and shrubs.\n",
    "5. static car: cars that are not moving, including static buses, trucks, automobiles, and tractors. Bicycles and motorcycles are not included.\n",
    "6. moving car: cars that are moving, including moving buses, trucks, automobiles, and tractors. Bicycles and motorcycles are not included.\n",
    "7. human: pedestrians, bikers, and all other humans occupied by different activities.\n",
    "8. clutter: all objects not belonging to any of the classes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "cls = [\n",
    "    \"Building\",\n",
    "    \"Tree\",\n",
    "    \"Clutter\",\n",
    "    \"Road\",\n",
    "    \"Vegetation\",\n",
    "    \"Static Car\",\n",
    "    \"Moving Car\",\n",
    "    \"Human\",\n",
    "]\n",
    "pixels = [30.436, 25.977, 17.120, 14.322, 9.464, 1.405, 1.115, 0.162]\n",
    "pix = pd.DataFrame({\"Classes\": cls, \"Pixel Number\": pixels})\n",
    "\n",
    "\n",
    "def plot_pixel():\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.set_palette(\n",
    "        [\n",
    "            \"#800000\",\n",
    "            \"#008000\",\n",
    "            \"#000000\",\n",
    "            \"#804080\",\n",
    "            \"#808000\",\n",
    "            \"#C000C0\",\n",
    "            \"#400080\",\n",
    "            \"#404000\",\n",
    "        ]\n",
    "    )\n",
    "    sns.barplot(x=\"Classes\", y=\"Pixel Number\", data=pix)\n",
    "    plt.title(\"Pixel Number Histogram\")\n",
    "    plt.ylabel(\"%\")\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_pixel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the pixels are from classes like building, tree, clutter, road, and low vegetation. Fewer pixels are from moving\n",
    "car and static car classes, which are both fewer than 1.5% of\n",
    "the total pixels. For human class, it is almost zero, fewer\n",
    "than 0.2% of the total pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll need helper functions for label image conversion from 3 channel RGB color image to 1 channel label index image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UAVidColorTransformer:\n",
    "    def __init__(self):\n",
    "        # color table.\n",
    "        self.clr_tab = self.createColorTable()\n",
    "        # id table.\n",
    "        id_tab = {}\n",
    "        for k, v in self.clr_tab.items():\n",
    "            id_tab[k] = self.clr2id(v)\n",
    "        self.id_tab = id_tab\n",
    "\n",
    "    def createColorTable(self):\n",
    "        clr_tab = {}\n",
    "        clr_tab[\"Clutter\"] = [0, 0, 0]\n",
    "        clr_tab[\"Building\"] = [128, 0, 0]\n",
    "        clr_tab[\"Road\"] = [128, 64, 128]\n",
    "        clr_tab[\"Static_Car\"] = [192, 0, 192]\n",
    "        clr_tab[\"Tree\"] = [0, 128, 0]\n",
    "        clr_tab[\"Vegetation\"] = [128, 128, 0]\n",
    "        clr_tab[\"Human\"] = [64, 64, 0]\n",
    "        clr_tab[\"Moving_Car\"] = [64, 0, 128]\n",
    "        return clr_tab\n",
    "\n",
    "    def colorTable(self):\n",
    "        return self.clr_tab\n",
    "\n",
    "    def clr2id(self, clr):\n",
    "        return clr[0] + clr[1] * 255 + clr[2] * 255 * 255\n",
    "\n",
    "    # transform to uint8 integer label\n",
    "    def transform(self, label, dtype=np.int32):\n",
    "        height, width = label.shape[:2]\n",
    "        # default value is index of clutter.\n",
    "        newLabel = np.zeros((height, width), dtype=dtype)\n",
    "        id_label = label.astype(np.int64)\n",
    "        id_label = (\n",
    "            id_label[:, :, 0] + id_label[:, :, 1] * 255 + id_label[:, :, 2] * 255 * 255\n",
    "        )\n",
    "        for tid, val in enumerate(self.id_tab.values()):\n",
    "            mask = id_label == val\n",
    "            newLabel[mask] = tid\n",
    "        return newLabel\n",
    "\n",
    "    # transform back to 3 channels uint8 label\n",
    "    def inverse_transform(self, label):\n",
    "        label_img = np.zeros(shape=(label.shape[0], label.shape[1], 3), dtype=np.uint8)\n",
    "        values = list(self.clr_tab.values())\n",
    "        for tid, val in enumerate(values):\n",
    "            mask = label == tid\n",
    "            label_img[mask] = val\n",
    "        return label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clrEnc = UAVidColorTransformer()\n",
    "\n",
    "\n",
    "def prepareTrainIDForDir(gtDirPath, saveDirPath):\n",
    "    gt_paths = [p for p in os.listdir(gtDirPath) if p.startswith(\"seq\")]\n",
    "    for pd in tqdm(gt_paths):\n",
    "        lbl_dir = osp.join(gtDirPath, pd, \"Labels\")\n",
    "        lbl_paths = os.listdir(lbl_dir)\n",
    "        if not osp.isdir(osp.join(saveDirPath, pd, \"TrainId\")):\n",
    "            os.makedirs(osp.join(saveDirPath, pd, \"TrainId\"))\n",
    "            assert osp.isdir(\n",
    "                osp.join(saveDirPath, pd, \"TrainId\")\n",
    "            ), \"Fail to create directory:%s\" % (osp.join(saveDirPath, pd, \"TrainId\"))\n",
    "        for lbl_p in lbl_paths:\n",
    "            lbl_path = osp.abspath(osp.join(lbl_dir, lbl_p))\n",
    "            trainId_path = osp.join(saveDirPath, pd, \"TrainId\", lbl_p)\n",
    "            gt = np.array(Image.open(lbl_path))\n",
    "            trainId = clrEnc.transform(gt, dtype=np.uint8)\n",
    "            Image.fromarray(trainId).save(trainId_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use joblib.Parallel, we can speedup ~3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prepareTrainIDForDir(\n",
    "    r\"data\\UAVidSemanticSegmentationDataset/train/train\", \"./trainlabels/\"\n",
    ")\n",
    "prepareTrainIDForDir(\n",
    "    r\"data\\UAVidSemanticSegmentationDataset/valid/valid\", \"./validlabels/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating label images, we’ll define lists of images and labels for our Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_image_list = sorted(\n",
    "    glob.glob(\n",
    "        pathname=r\"data\\UAVidSemanticSegmentationDataset/train/train/*/Images/*.png\",\n",
    "        recursive=True,\n",
    "    )\n",
    ")\n",
    "train_mask_list = sorted(\n",
    "    glob.glob(pathname=\"./trainlabels/*/TrainId/*.png\", recursive=True)\n",
    ")\n",
    "valid_image_list = sorted(\n",
    "    glob.glob(\n",
    "        pathname=\"../input/uavid-semantic-segmentation-dataset/valid/valid/*/Images/*.png\",\n",
    "        recursive=True,\n",
    "    )\n",
    ")\n",
    "valid_mask_list = sorted(\n",
    "    glob.glob(pathname=\"./validlabels/*/TrainId/*.png\", recursive=True)\n",
    ")\n",
    "print(train_image_list[42])\n",
    "print(train_mask_list[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed and mix precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)\n",
    "is_fp16_used = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Dataset, we can read images, extract values of classes from segmentation mask, apply augmentation and pre-processing transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "\n",
    "class Dataset(BaseDataset):\n",
    "\n",
    "    CLASSES = [\n",
    "        \"clutter\",\n",
    "        \"building\",\n",
    "        \"road\",\n",
    "        \"static_car\",\n",
    "        \"tree\",\n",
    "        \"vegetation\",\n",
    "        \"human\",\n",
    "        \"moving_car\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_list,\n",
    "        masks_list,\n",
    "        classes=None,\n",
    "        augmentation=None,\n",
    "        preprocessing=None,\n",
    "    ):\n",
    "        self.images_list = images_list\n",
    "        self.masks_list = masks_list\n",
    "        self.classes = classes\n",
    "\n",
    "        # convert str names to class values on masks\n",
    "        if self.classes is not None:\n",
    "            self.class_values = (\n",
    "                np.array([self.CLASSES.index(cls.lower()) for cls in classes]) / 255\n",
    "            )\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_list[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_list[i], 0)\n",
    "        mask = mask.astype(\"float\") / 255\n",
    "\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        if self.classes is not None:\n",
    "            masks = [(mask == v) for v in self.class_values]\n",
    "            mask = np.stack(masks, axis=-1).astype(\"float\")\n",
    "        else:\n",
    "            mask = np.expand_dims(mask, 2)\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize(image, mask, label=None, truth=None, augment=False):\n",
    "    if truth is None:\n",
    "        plt.figure(figsize=(14, 20))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        if augment == False:\n",
    "            plt.title(f\"{'Original Image'}\")\n",
    "        else:\n",
    "            plt.title(f\"{'Augmented Image'}\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask)\n",
    "        if label is not None:\n",
    "            plt.title(f\"{label.capitalize()}\")\n",
    "\n",
    "    else:\n",
    "        plt.figure(figsize=(26, 36))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"{'Original Image'}\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title(f\"{'Prediction'}\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(truth)\n",
    "        plt.title(f\"{'Ground Truth'}\")\n",
    "\n",
    "\n",
    "def visualize_overlay(image, mask, truth_path=None):\n",
    "    if truth_path is None:\n",
    "        plt.figure(figsize=(26, 36))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"{'Original Image'}\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title(f\"{'Prediction'}\")\n",
    "\n",
    "    else:\n",
    "        truth = Image.open(truth_path)\n",
    "        plt.figure(figsize=(26, 36))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"{'Original Image'}\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title(f\"{'Prediction'}\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(truth)\n",
    "        plt.title(f\"{'Ground Truth'}\")\n",
    "\n",
    "\n",
    "def visualize_prediction(image, mask):\n",
    "    plt.figure(figsize=(26, 36))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{'Original Image'}\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask)\n",
    "    plt.title(f\"{'Prediction'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing segmentation masks for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"clutter\",\n",
    "    \"building\",\n",
    "    \"road\",\n",
    "    \"static_car\",\n",
    "    \"tree\",\n",
    "    \"vegetation\",\n",
    "    \"human\",\n",
    "    \"moving_car\",\n",
    "]\n",
    "for label in labels:\n",
    "    dataset = Dataset(train_image_list, train_mask_list, classes=[label])\n",
    "\n",
    "    image, mask = dataset[4]\n",
    "    visualize(image=image, mask=mask.squeeze(), label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll resize images to `576*1024` to keep 9:16 ratio. \n",
    "\n",
    "Augmentation list:\n",
    "* *HorizontalFlip*\n",
    "* *OneOf(RandomBrightnessContrast, CLAHE, HueSaturationValue)*\n",
    "* *IAAAdditiveGaussianNoise* with 0.2 probability\n",
    "\n",
    "*Note:* For better result we could crop each image into 16 evenly distributed smaller(1280*720) overlapped images that cover the whole image for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.Resize(576, 1024, p=1),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(\n",
    "                    brightness_limit=0.4, contrast_limit=0.4, p=1\n",
    "                ),\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        albu.Resize(576, 1024, p=1),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype(\"float32\")\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing augmented images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"clutter\",\n",
    "    \"building\",\n",
    "    \"road\",\n",
    "    \"static_car\",\n",
    "    \"tree\",\n",
    "    \"vegetation\",\n",
    "    \"human\",\n",
    "    \"moving_car\",\n",
    "]\n",
    "for label in labels:\n",
    "    augmented_dataset = Dataset(\n",
    "        train_image_list,\n",
    "        train_mask_list,\n",
    "        augmentation=get_training_augmentation(),\n",
    "        classes=[label],\n",
    "    )\n",
    "\n",
    "    # same image with different random transforms\n",
    "    image, mask = augmented_dataset[8]\n",
    "    visualize(image=image, mask=mask.squeeze(), label=label, augment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments, we'll use FPN model with EfficientnetB3 encoder. The motivation was to select best model in Memory Consumption, Accuracy trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "ENCODER = \"efficientnet-b3\"\n",
    "ENCODER_WEIGHTS = \"imagenet\"\n",
    "ACTIVATION = \"sigmoid\"\n",
    "CLASSES = [\n",
    "    \"clutter\",\n",
    "    \"building\",\n",
    "    \"road\",\n",
    "    \"static_car\",\n",
    "    \"tree\",\n",
    "    \"vegetation\",\n",
    "    \"human\",\n",
    "    \"moving_car\",\n",
    "]\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll define Dataloaders and set batch size to 6, because of memory limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    train_image_list,\n",
    "    train_mask_list,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    valid_image_list,\n",
    "    valid_mask_list,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting optimization level: **01** - Mixed Precision (recommended for typical use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if is_fp16_used:\n",
    "    fp16_params = dict(opt_level=\"O1\")\n",
    "else:\n",
    "    fp16_params = None\n",
    "\n",
    "print(f\"FP16 params: {fp16_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment settings:\n",
    "* Loss: BCEDiceLoss with 0.5 contibution of BCE and Dice\n",
    "* Optimizer: Lookahead(improves the learning stability and lowers the variance of its inner optimizer)\n",
    "* Scheduler: OneCycleLRWithWarmup with 2 warmup steps\n",
    "* Initial learning rate is set to 1e-3, and 1e-4 on encoder. Number of epochs to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn import BCEDiceLoss, RAdam, Lookahead, OneCycleLRWithWarmup\n",
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "logdir = \"./logs\"\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "base_optimizer = RAdam(\n",
    "    [\n",
    "        {\"params\": model.decoder.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.encoder.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.segmentation_head.parameters(), \"lr\": learning_rate},\n",
    "    ]\n",
    ")\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "criterion = BCEDiceLoss(activation=None)\n",
    "runner = SupervisedRunner()\n",
    "scheduler = OneCycleLRWithWarmup(\n",
    "    optimizer,\n",
    "    num_steps=num_epochs,\n",
    "    lr_range=(0.0016, 0.0000001),\n",
    "    init_lr=learning_rate,\n",
    "    warmup_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing callbacks for metrics and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import (\n",
    "    IouCallback,\n",
    "    WandbLogger,\n",
    "    EarlyStoppingCallback,\n",
    "    ClasswiseIouCallback,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    IouCallback(activation=\"none\"),\n",
    "    ClasswiseIouCallback(classes=CLASSES, activation=\"none\"),\n",
    "    EarlyStoppingCallback(patience=7, metric=\"iou\", minimize=False),\n",
    "    WandbLogger(project=\"Project_Name\", name=\"Run_Name\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training, set main_metric to **'iou'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    callbacks=callbacks,\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    # save our best checkpoint by IoU metric\n",
    "    main_metric=\"iou\",\n",
    "    # IoU needs to be maximized.\n",
    "    minimize_metric=False,\n",
    "    # for FP16. It uses the variable from the very first cell\n",
    "    fp16=fp16_params,\n",
    "    # prints train logs\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/qjGFnbw/single.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cls = [\n",
    "    \"Building\",\n",
    "    \"Tree\",\n",
    "    \"Clutter\",\n",
    "    \"Road\",\n",
    "    \"Vegetation\",\n",
    "    \"Static Car\",\n",
    "    \"Moving Car\",\n",
    "    \"Human\",\n",
    "]\n",
    "s_iou = [85.64, 72.85, 55.86, 69.78, 54.93, 0, 0, 0]\n",
    "sing_iou = pd.DataFrame({\"Classes\": cls, \"IoU\": s_iou})\n",
    "single_iou = sing_iou.pivot_table(values=\"IoU\", columns=\"Classes\")\n",
    "single_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(loaders[\"valid\"]))\n",
    "dataset = Dataset(\n",
    "    valid_image_list, valid_mask_list, augmentation=get_validation_augmentation()\n",
    ")\n",
    "image, _ = dataset[0]\n",
    "truth_path = (\n",
    "    \"../input/uavid-semantic-segmentation-dataset/valid/valid/seq16/Labels/000000.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import mask_to_overlay_image\n",
    "\n",
    "pred = mask_to_overlay_image(image=image, masks=single[0], threshold=0.4)\n",
    "visualize_overlay(image, pred, truth_path=truth_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we’ve shown strong results in most pixels classes. But we’re not able to identify cars and people. \n",
    "\n",
    "To address this problem, we create 3 additional models for static car, moving car and human classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clutter, Building, Road, Tree, Vegetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New experiment settings:\n",
    "* Loss: The same\n",
    "* Optimizer: The same\n",
    "* Scheduler: ReduceLROnPlateau with patience 3 and factor 0.3\n",
    "* Number of epochs changed to 40. Added weight decay. Learning rate is set to 1.5e-3. Batch size reduced to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "ENCODER = \"efficientnet-b3\"\n",
    "ENCODER_WEIGHTS = \"imagenet\"\n",
    "ACTIVATION = \"sigmoid\"\n",
    "CLASSES = [\"clutter\", \"building\", \"road\", \"tree\", \"vegetation\"]\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "logdir = \"./logs\"\n",
    "num_epochs = 40\n",
    "learning_rate = 1.5e-3\n",
    "base_optimizer = RAdam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": model.decoder.parameters(),\n",
    "            \"lr\": learning_rate,\n",
    "            \"weight_decay\": 1e-3,\n",
    "        },\n",
    "        {\"params\": model.encoder.parameters(), \"lr\": 1e-4, \"weight_decay\": 1e-4},\n",
    "        {\"params\": model.segmentation_head.parameters(), \"lr\": learning_rate},\n",
    "    ]\n",
    ")\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.3, patience=3, mode=\"max\")\n",
    "criterion = BCEDiceLoss(activation=None)\n",
    "runner = SupervisedRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/hRhzVBf/cls5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cls = [\"Building\", \"Tree\", \"Clutter\", \"Road\", \"Vegetation\"]\n",
    "s_iou = [85.64, 72.85, 55.86, 69.78, 54.93]\n",
    "cls5_iou = [86.79, 74.16, 58.01, 71.08, 53.59]\n",
    "sing_iou = pd.DataFrame([s_iou, cls5_iou], columns=cls, index=[\"single\", \"top5\"])\n",
    "sing_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the table, we’ve improved previous results in 4 of 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred5 = mask_to_overlay_image(image=image, masks=cls5[0], threshold=0.4)\n",
    "visualize_overlay(image, pred5, truth_path=truth_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try several learning rates: 3e-4, 7e-4, 1e-3, 1.5e-3, 3e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/XphB6s2/mc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    valid_image_list,\n",
    "    valid_mask_list,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    classes=[\"moving_car\"],\n",
    ")\n",
    "image, ground_truth = dataset[0]\n",
    "\n",
    "grs2 = mask_to_overlay_image(image=image, masks=mc[0], threshold=0.4)\n",
    "visualize(image, grs2, truth=ground_truth.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/cXKMJRS/sc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    valid_image_list,\n",
    "    valid_mask_list,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    classes=[\"static_car\"],\n",
    ")\n",
    "image, ground_truth = dataset[0]\n",
    "\n",
    "grs3 = mask_to_overlay_image(image=image, masks=sc[0], threshold=0.4)\n",
    "visualize(image, grs3, truth=ground_truth.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ibb.co/VVDpYxt/hum.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    valid_image_list,\n",
    "    valid_mask_list,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    classes=[\"human\"],\n",
    ")\n",
    "image, ground_truth = dataset[0]\n",
    "\n",
    "grs4 = mask_to_overlay_image(image=image, masks=hum[0], threshold=0.4)\n",
    "visualize(image, grs4, truth=ground_truth.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cls = [\n",
    "    \"Building\",\n",
    "    \"Tree\",\n",
    "    \"Clutter\",\n",
    "    \"Road\",\n",
    "    \"Vegetation\",\n",
    "    \"Static Car\",\n",
    "    \"Moving Car\",\n",
    "    \"Human\",\n",
    "    \"mIoU\",\n",
    "]\n",
    "ensemble = [86.79, 74.16, 58.01, 71.08, 53.59, 51.34, 39.27, 22.21, 57.06]\n",
    "final = pd.DataFrame([ensemble], columns=cls, index=[\"ensemble\"])\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    valid_image_list, valid_mask_list, augmentation=get_validation_augmentation()\n",
    ")\n",
    "image, _ = dataset[0]\n",
    "image1, _ = dataset[1]\n",
    "image2, _ = dataset[2]\n",
    "image3, _ = dataset[3]\n",
    "image4, _ = dataset[4]\n",
    "images = [image, image1, image2, image3, image4]\n",
    "full = [\n",
    "    [\n",
    "        cls5[i][0],\n",
    "        cls5[i][1],\n",
    "        cls5[i][2],\n",
    "        sc[i][0],\n",
    "        cls5[i][3],\n",
    "        cls5[i][4],\n",
    "        hum[i][0],\n",
    "        mc[i][0],\n",
    "    ]\n",
    "    for i in range(5)\n",
    "]\n",
    "full_truth1 = [\n",
    "    mask_to_overlay_image(image=images[i], masks=full[i], threshold=0.4)\n",
    "    for i in range(5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    visualize_prediction(images[i], full_truth1[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
