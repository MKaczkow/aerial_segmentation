{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from torchvision.transforms import Resize, InterpolationMode, ToPILImage\n",
    "from torchmetrics import JaccardIndex, Precision, Recall, F1Score\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from src.evaluation.evaluate_result import evaluate_result\n",
    "from src.callbacks.SaveRandomImagesCallback import SaveRandomImagesCallback\n",
    "from src.callbacks.SaveTestPreds import SaveTestPreds\n",
    "from src.datasets.INRIAAerialImageLabellingDatasetPatches import (\n",
    "    INRIAAerialImageLabellingDatasetPatches,\n",
    ")\n",
    "from src.utils import view_and_save_images_shapes\n",
    "from src.datasets.utils.ResizeToDivisibleBy32 import ResizeToDivisibleBy32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if you get some cryptic CUDA error, set device to \"cpu\" and try again\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "BATCH_SIZE = 16\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 576\n",
    "SAVE_VAL_DIR = \"outputs/INRIA-patches/val\"\n",
    "SAVE_TEST_DIR = \"outputs/INRIA-patches/test\"\n",
    "INRIA_PATCHES_DATASET_PATH = \"data/INRIAAerialImageLabellingDatasetPatches\"  # home PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "labeled_dataset = INRIAAerialImageLabellingDatasetPatches(\n",
    "    INRIA_PATCHES_DATASET_PATH,\n",
    "    split=\"train\",\n",
    "    transforms=[ResizeToDivisibleBy32()],\n",
    ")\n",
    "print(len(labeled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14400\n"
     ]
    }
   ],
   "source": [
    "test_dataset = INRIAAerialImageLabellingDatasetPatches(\n",
    "    INRIA_PATCHES_DATASET_PATH,\n",
    "    split=\"test\",\n",
    "    transforms=[ResizeToDivisibleBy32()],\n",
    ")\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_dataset = INRIAAerialImageLabellingDatasetPatches(\n",
    "    INRIA_PATCHES_DATASET_PATH,\n",
    "    split=\"train\",\n",
    "    transforms=[ResizeToDivisibleBy32()],\n",
    ")\n",
    "print(len(sanity_check_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check_dataloader = DataLoader(\n",
    "    sanity_check_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "torch.Size([16, 3, 512, 512])\n",
      "{0.0: 10306, 0.003921569: 16872, 0.007843138: 8924, 0.011764706: 3758, 0.015686275: 2456, 0.019607844: 2610, 0.023529412: 2772, 0.02745098: 2495, 0.03137255: 2383, 0.03529412: 2151, 0.039215688: 2190, 0.043137256: 2658, 0.047058824: 3352, 0.050980393: 3261, 0.05490196: 3454, 0.05882353: 3791, 0.0627451: 4469, 0.06666667: 5606, 0.07058824: 8516, 0.07450981: 13283, 0.078431375: 17363, 0.08235294: 18285, 0.08627451: 18593, 0.09019608: 18903, 0.09411765: 21447, 0.09803922: 23591, 0.101960786: 23067, 0.105882354: 22800, 0.10980392: 22157, 0.11372549: 25089, 0.11764706: 26111, 0.12156863: 27646, 0.1254902: 31428, 0.12941177: 34024, 0.13333334: 39943, 0.13725491: 44055, 0.14117648: 45839, 0.14509805: 47214, 0.14901961: 50342, 0.15294118: 52682, 0.15686275: 53640, 0.16078432: 55762, 0.16470589: 56878, 0.16862746: 58460, 0.17254902: 61373, 0.1764706: 63883, 0.18039216: 65593, 0.18431373: 68307, 0.1882353: 71636, 0.19215687: 72752, 0.19607843: 76906, 0.2: 80412, 0.20392157: 85857, 0.20784314: 82222, 0.21176471: 79806, 0.21568628: 78958, 0.21960784: 81361, 0.22352941: 86068, 0.22745098: 89623, 0.23137255: 91128, 0.23529412: 92108, 0.23921569: 90875, 0.24313726: 90869, 0.24705882: 89615, 0.2509804: 89688, 0.25490198: 87737, 0.25882354: 85363, 0.2627451: 85991, 0.26666668: 88904, 0.27058825: 91959, 0.27450982: 90095, 0.2784314: 90642, 0.28235295: 90124, 0.28627452: 91099, 0.2901961: 91039, 0.29411766: 91769, 0.29803923: 92010, 0.3019608: 92723, 0.30588236: 92787, 0.30980393: 90962, 0.3137255: 90198, 0.31764707: 95094, 0.32156864: 97922, 0.3254902: 98388, 0.32941177: 97587, 0.33333334: 100692, 0.3372549: 101643, 0.34117648: 103866, 0.34509805: 103676, 0.34901962: 103855, 0.3529412: 103786, 0.35686275: 104189, 0.36078432: 104603, 0.3647059: 112115, 0.36862746: 117657, 0.37254903: 128621, 0.3764706: 142980, 0.38039216: 143508, 0.38431373: 143846, 0.3882353: 150538, 0.39215687: 155452, 0.39607844: 158786, 0.4: 155677, 0.40392157: 145378, 0.40784314: 136933, 0.4117647: 130291, 0.41568628: 122093, 0.41960785: 114835, 0.42352942: 108227, 0.42745098: 101147, 0.43137255: 97690, 0.43529412: 93517, 0.4392157: 90543, 0.44313726: 88201, 0.44705883: 83478, 0.4509804: 79770, 0.45490196: 80218, 0.45882353: 80475, 0.4627451: 82595, 0.46666667: 86126, 0.47058824: 90373, 0.4745098: 97707, 0.47843137: 102998, 0.48235294: 104361, 0.4862745: 101741, 0.49019608: 97618, 0.49411765: 92091, 0.49803922: 82562, 0.5019608: 75213, 0.5058824: 72877, 0.50980395: 69529, 0.5137255: 66019, 0.5176471: 63410, 0.52156866: 62548, 0.5254902: 60012, 0.5294118: 57950, 0.53333336: 57417, 0.5372549: 55608, 0.5411765: 54373, 0.54509807: 54488, 0.54901963: 52540, 0.5529412: 51793, 0.5568628: 54134, 0.56078434: 53607, 0.5647059: 52223, 0.5686275: 50331, 0.57254905: 49943, 0.5764706: 48741, 0.5803922: 48045, 0.58431375: 46756, 0.5882353: 47717, 0.5921569: 48163, 0.59607846: 45892, 0.6: 44199, 0.6039216: 44628, 0.60784316: 44546, 0.6117647: 43957, 0.6156863: 43248, 0.61960787: 44066, 0.62352943: 43889, 0.627451: 42721, 0.6313726: 41753, 0.63529414: 39922, 0.6392157: 38727, 0.6431373: 39931, 0.64705884: 40492, 0.6509804: 39354, 0.654902: 37229, 0.65882355: 36525, 0.6627451: 36997, 0.6666667: 35738, 0.67058825: 34611, 0.6745098: 35022, 0.6784314: 34800, 0.68235296: 33482, 0.6862745: 32794, 0.6901961: 32720, 0.69411767: 33028, 0.69803923: 31872, 0.7019608: 30489, 0.7058824: 29706, 0.70980394: 29692, 0.7137255: 29867, 0.7176471: 27899, 0.72156864: 27280, 0.7254902: 26623, 0.7294118: 26370, 0.73333335: 25147, 0.7372549: 25041, 0.7411765: 24040, 0.74509805: 24175, 0.7490196: 23107, 0.7529412: 22045, 0.75686276: 21051, 0.7607843: 19936, 0.7647059: 20488, 0.76862746: 21724, 0.77254903: 21134, 0.7764706: 20092, 0.78039217: 18451, 0.78431374: 17257, 0.7882353: 16806, 0.7921569: 16336, 0.79607844: 15609, 0.8: 14560, 0.8039216: 14288, 0.80784315: 13849, 0.8117647: 13011, 0.8156863: 12324, 0.81960785: 12279, 0.8235294: 11759, 0.827451: 11402, 0.83137256: 11058, 0.8352941: 10840, 0.8392157: 10543, 0.84313726: 10283, 0.84705883: 10074, 0.8509804: 9699, 0.85490197: 9615, 0.85882354: 9125, 0.8627451: 9157, 0.8666667: 8439, 0.87058824: 8285, 0.8745098: 8007, 0.8784314: 7266, 0.88235295: 7098, 0.8862745: 6840, 0.8901961: 6499, 0.89411765: 6094, 0.8980392: 5919, 0.9019608: 5776, 0.90588236: 5468, 0.9098039: 5697, 0.9137255: 5777, 0.91764706: 6441, 0.92156863: 6580, 0.9254902: 6418, 0.92941177: 5688, 0.93333334: 5064, 0.9372549: 4765, 0.9411765: 4611, 0.94509804: 4597, 0.9490196: 4325, 0.9529412: 4078, 0.95686275: 3848, 0.9607843: 3813, 0.9647059: 3956, 0.96862745: 3849, 0.972549: 4335, 0.9764706: 4311, 0.98039216: 4357, 0.9843137: 5495, 0.9882353: 5182, 0.99215686: 4866, 0.99607843: 6845, 1.0: 23995}\n",
      "\n",
      "masks\n",
      "torch.Size([16, 1, 512, 512])\n",
      "{0: 3370901, 255: 823403}\n"
     ]
    }
   ],
   "source": [
    "for images, masks in sanity_check_dataloader:\n",
    "    print(\"images\")\n",
    "    print(images.shape)\n",
    "    unique, counts = np.unique(images, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print()\n",
    "    print(\"masks\")\n",
    "    print(masks.shape)\n",
    "    unique, counts = np.unique(masks, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_and_save_images_shapes(\n",
    "#     sanity_check_dataloader, \"inria_patches_images_shapes\", verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14400 3600 14400\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(labeled_dataset))\n",
    "val_size = len(labeled_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    labeled_dataset, [train_size, val_size]\n",
    ")\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 512, 512])\n",
      "torch.Size([16, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BATCH_SIZE == 1:\n",
    "    img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BATCH_SIZE == 1:\n",
    "    msk = to_pil_transform(masks.squeeze()).convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-3):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.criterion = smp.losses.MCCLoss()\n",
    "        self.train_iou = JaccardIndex(num_classes=2, task=\"binary\")\n",
    "        self.val_iou = JaccardIndex(num_classes=2, task=\"binary\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x.to(device))\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        masks = torch.div(masks, 255).float()\n",
    "        preds = self(images)\n",
    "        loss = self.criterion(preds, masks)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=True)\n",
    "        self.log(\"train_iou\", self.train_iou(preds, masks), on_epoch=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        masks = torch.div(masks, 255).float()\n",
    "        preds = self(images)\n",
    "        loss = self.criterion(preds, masks)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=True)\n",
    "        self.log(\"val_iou\", self.val_iou(preds, masks), on_epoch=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # just here to activate the test_epoch_end\n",
    "        # callback SaveTestPreds starts on_test_epoch_end\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,  # model output channels (number of classes in your dataset)\n",
    "    activation=\"sigmoid\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.UnetPlusPlus(\n",
    "#     encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,  # model output channels (number of classes in your dataset)\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.DeepLabV3(\n",
    "#     encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,\n",
    "#     activation=\"sigmoid\"\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.DeepLabV3Plus(\n",
    "#     encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#     encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "#     in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#     classes=1,\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_model = SegmentationModel(model)\n",
    "segmentation_model = SegmentationModel.load_from_checkpoint(\n",
    "    r\"lightning_logs\\inria_patches_segmentation_model\\version_1\\checkpoints\\epoch=29-step=27000.ckpt\",\n",
    "    model=model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", save_top_k=-1, mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(\"lightning_logs\", name=\"inria_patches_segmentation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = SaveRandomImagesCallback(save_dir=SAVE_VAL_DIR)\n",
    "save_test_preds_callback = SaveTestPreds(save_dir=SAVE_TEST_DIR)\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[\n",
    "        model_checkpoint_callback,\n",
    "        save_callback,\n",
    "        save_test_preds_callback,\n",
    "        early_stopping_callback\n",
    "    ],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "# DEBUG\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=1,\n",
    "#     callbacks=[model_checkpoint_callback, save_callback],\n",
    "#     logger=logger,\n",
    "#     accelerator=\"cpu\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | Unet               | 14.3 M\n",
      "1 | criterion | MCCLoss            | 0     \n",
      "2 | train_iou | BinaryJaccardIndex | 0     \n",
      "3 | val_iou   | BinaryJaccardIndex | 0     \n",
      "-------------------------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.313    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 900/900 [07:57<00:00,  1.89it/s, v_num=3]        \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    segmentation_model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at lightning_logs\\inria_patches_segmentation_model\\version_3\\checkpoints\\epoch=4-step=4500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at lightning_logs\\inria_patches_segmentation_model\\version_3\\checkpoints\\epoch=4-step=4500.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 900/900 [00:58<00:00, 15.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(ckpt_path=\"best\", dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
