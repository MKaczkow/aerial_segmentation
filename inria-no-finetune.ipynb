{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from src.models.BaselineModel import BaselineModel\n",
    "from src.evaluation.evaluate_result import evaluate_result\n",
    "from src.datasets.INRIAAerialImageLabellingDataset import (\n",
    "    INRIAAerialImageLabellingDataset,\n",
    ")\n",
    "\n",
    "from src.datasets.utils.ResizeToDivisibleBy32 import ResizeToDivisibleBy32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "BATCH_SIZE = 1\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 576\n",
    "INRIA_DATASET_PATH = \"data/INRIAAerialImageLabellingDataset\"  # home PC\n",
    "# INRIA_DATASET_PATH = \"data/TestSubsets/INRIAAerialImageLabellingDataset\"  # laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/INRIAAerialImageLabellingDataset\\train\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "labeled_dataset = INRIAAerialImageLabellingDataset(\n",
    "    INRIA_DATASET_PATH,\n",
    "    transforms=[\n",
    "        Resize(IMAGE_SIZE),\n",
    "        # ResizeToDivisibleBy32()\n",
    "    ],\n",
    ")\n",
    "print(len(labeled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/INRIAAerialImageLabellingDataset\\test\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "test_dataset = INRIAAerialImageLabellingDataset(\n",
    "    INRIA_DATASET_PATH,\n",
    "    split=\"test\",\n",
    "    transforms=[\n",
    "        Resize(IMAGE_SIZE),\n",
    "        # ResizeToDivisibleBy32()\n",
    "    ],\n",
    ")\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,  # model output channels (number of classes in your dataset)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = (output > 0.5).float()\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = (output_baseline > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 331764, 255: 12}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(outp, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 331764, 1.0: 12}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(output.cpu(), return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_outp = to_pil_transform(output_baseline.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 166195, 255: 165581}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(rand_outp, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(5.2252e-05),\n",
       " 'f1': tensor(0.0001),\n",
       " 'accuracy': tensor(-52.7583),\n",
       " 'recall': tensor(3.6287e-05)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.9774),\n",
       " 'f1': tensor(0.9886),\n",
       " 'accuracy': tensor(0.3809),\n",
       " 'recall': tensor(0.4984)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output_baseline, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(-1.0087),\n",
       " 'f1': tensor(231.4211),\n",
       " 'accuracy': tensor(24777.6680),\n",
       " 'recall': tensor(231.1913)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(masks, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "metrics_dict = {\n",
    "    \"iou\": [],\n",
    "    \"f1\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        output = model(images.to(device))\n",
    "        output = (output > 0.5).float()\n",
    "\n",
    "        iter_metrics = evaluate_result(output, masks)\n",
    "        for key in metrics_dict.keys():\n",
    "            metrics_dict[key].append(iter_metrics[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iou': [tensor(0.), tensor(0.), tensor(0.0009), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(2.7233e-05), tensor(0.), tensor(0.), tensor(1.9778e-05), tensor(0.), tensor(0.), tensor(0.), tensor(5.7072e-05), tensor(3.4782e-05), tensor(5.4366e-06), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.1338e-05), tensor(6.0442e-06), tensor(2.0260e-05), tensor(0.), tensor(4.6689e-07), tensor(2.1716e-05), tensor(0.0004), tensor(0.), tensor(0.), tensor(4.7792e-06), tensor(0.), tensor(1.1329e-06), tensor(7.8165e-06), tensor(5.1175e-06), tensor(0.), tensor(0.), tensor(0.), tensor(3.0650e-06), tensor(5.2252e-05), tensor(0.), tensor(1.3084e-07), tensor(0.), tensor(0.), tensor(0.), tensor(0.0002), tensor(0.), tensor(1.5271e-05), tensor(0.), tensor(5.1855e-06), tensor(0.), tensor(0.0001), tensor(6.4458e-05), tensor(0.), tensor(0.), tensor(0.), tensor(5.1268e-05), tensor(3.5380e-05), tensor(9.9601e-05), tensor(0.), tensor(0.), tensor(0.), tensor(1.2125e-05), tensor(0.), tensor(0.), tensor(1.5915e-07), tensor(0.), tensor(3.4816e-05), tensor(0.), tensor(0.), tensor(1.6322e-07), tensor(1.0170e-05), tensor(4.7380e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(9.2667e-05), tensor(1.4296e-05), tensor(0.), tensor(0.), tensor(4.2486e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.6862e-07), tensor(8.3512e-05), tensor(0.), tensor(0.), tensor(0.), tensor(1.9523e-05), tensor(0.), tensor(0.), tensor(1.2650e-05), tensor(1.6139e-05), tensor(2.4632e-05), tensor(0.), tensor(1.4702e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(8.1218e-05), tensor(9.5851e-06), tensor(0.), tensor(9.8238e-05), tensor(0.), tensor(8.7790e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0003), tensor(0.), tensor(1.2839e-05), tensor(1.2671e-05), tensor(3.0537e-07), tensor(2.9356e-05), tensor(0.), tensor(4.0461e-05), tensor(0.), tensor(3.2735e-05), tensor(0.), tensor(0.), tensor(0.), tensor(4.2051e-07), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0003), tensor(0.), tensor(0.), tensor(4.2755e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.0006), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0057), tensor(0.), tensor(0.), tensor(0.), tensor(2.8299e-05), tensor(0.), tensor(0.0003), tensor(0.), tensor(7.9632e-06), tensor(7.4720e-06), tensor(2.0735e-05), tensor(0.), tensor(0.), tensor(0.0002), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(7.9388e-07), tensor(0.0012), tensor(0.), tensor(0.)], 'f1': [tensor(0.), tensor(0.), tensor(0.0018), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(5.4464e-05), tensor(0.), tensor(0.), tensor(3.9555e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.0001), tensor(6.9561e-05), tensor(1.0873e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(2.2676e-05), tensor(1.2088e-05), tensor(4.0518e-05), tensor(0.), tensor(9.3379e-07), tensor(4.3431e-05), tensor(0.0009), tensor(0.), tensor(0.), tensor(9.5584e-06), tensor(0.), tensor(2.2658e-06), tensor(1.5633e-05), tensor(1.0235e-05), tensor(0.), tensor(0.), tensor(0.), tensor(6.1299e-06), tensor(0.0001), tensor(0.), tensor(2.6169e-07), tensor(0.), tensor(0.), tensor(0.), tensor(0.0004), tensor(0.), tensor(3.0542e-05), tensor(0.), tensor(1.0371e-05), tensor(0.), tensor(0.0003), tensor(0.0001), tensor(0.), tensor(0.), tensor(0.), tensor(0.0001), tensor(7.0757e-05), tensor(0.0002), tensor(0.), tensor(0.), tensor(0.), tensor(2.4250e-05), tensor(0.), tensor(0.), tensor(3.1830e-07), tensor(0.), tensor(6.9630e-05), tensor(0.), tensor(0.), tensor(3.2643e-07), tensor(2.0341e-05), tensor(9.4755e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0002), tensor(2.8591e-05), tensor(0.), tensor(0.), tensor(8.4968e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(3.3724e-07), tensor(0.0002), tensor(0.), tensor(0.), tensor(0.), tensor(3.9045e-05), tensor(0.), tensor(0.), tensor(2.5299e-05), tensor(3.2277e-05), tensor(4.9264e-05), tensor(0.), tensor(2.9405e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0002), tensor(1.9170e-05), tensor(0.), tensor(0.0002), tensor(0.), tensor(0.0002), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0006), tensor(0.), tensor(2.5677e-05), tensor(2.5341e-05), tensor(6.1074e-07), tensor(5.8709e-05), tensor(0.), tensor(8.0918e-05), tensor(0.), tensor(6.5469e-05), tensor(0.), tensor(0.), tensor(0.), tensor(8.4103e-07), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0005), tensor(0.), tensor(0.), tensor(8.5507e-05), tensor(0.), tensor(0.), tensor(0.), tensor(0.0012), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.0113), tensor(0.), tensor(0.), tensor(0.), tensor(5.6597e-05), tensor(0.), tensor(0.0006), tensor(0.), tensor(1.5926e-05), tensor(1.4944e-05), tensor(4.1468e-05), tensor(0.), tensor(0.), tensor(0.0004), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(1.5878e-06), tensor(0.0023), tensor(0.), tensor(0.)], 'accuracy': [tensor(-6.3363), tensor(-17.9159), tensor(-4.8785), tensor(-78.1926), tensor(-17.8508), tensor(-13.5645), tensor(-36.0020), tensor(-75.8091), tensor(-56.2401), tensor(-4.8183), tensor(-87.6934), tensor(-3.8958), tensor(-58.1049), tensor(-68.9874), tensor(-44.6796), tensor(-29.6755), tensor(-28.3831), tensor(-40.7177), tensor(-47.5918), tensor(-27.6059), tensor(-7.0169), tensor(0.4947), tensor(-11.3847), tensor(-22.1283), tensor(-66.7877), tensor(-19.9441), tensor(-59.5495), tensor(-59.7806), tensor(-44.1892), tensor(-82.5527), tensor(-22.7883), tensor(-9.0559), tensor(-33.4966), tensor(-25.4876), tensor(-5.7391), tensor(-52.2095), tensor(-123.5502), tensor(-78.5105), tensor(-23.7331), tensor(-21.3098), tensor(-46.9337), tensor(-84.5557), tensor(-52.7583), tensor(-46.6460), tensor(-22.0356), tensor(-43.7239), tensor(-1.3901), tensor(-95.6200), tensor(-29.8685), tensor(-75.4023), tensor(-49.3294), tensor(-42.4907), tensor(-53.0566), tensor(-71.1521), tensor(-18.4230), tensor(-10.7829), tensor(-6.4928), tensor(-49.7341), tensor(-35.6280), tensor(-13.9910), tensor(-64.6812), tensor(-39.0017), tensor(-45.0103), tensor(-7.0257), tensor(-13.8207), tensor(-62.1376), tensor(-55.1667), tensor(-8.7881), tensor(-55.8162), tensor(-3.4083), tensor(-68.4277), tensor(-32.1847), tensor(-40.2052), tensor(-35.9339), tensor(-46.7128), tensor(-31.1244), tensor(-10.5745), tensor(-19.5701), tensor(-58.7596), tensor(-59.0292), tensor(-36.6243), tensor(-12.3009), tensor(-11.0808), tensor(-43.6499), tensor(-21.8742), tensor(-5.8492), tensor(-57.0536), tensor(-75.7432), tensor(-29.5640), tensor(-3.4303), tensor(-55.8939), tensor(-40.9726), tensor(-85.2670), tensor(-52.2675), tensor(-40.9357), tensor(-20.1664), tensor(-56.2387), tensor(-16.7941), tensor(-43.4059), tensor(-39.5495), tensor(-16.8747), tensor(-39.0944), tensor(-96.7172), tensor(-58.3338), tensor(-3.4674), tensor(-38.3678), tensor(-83.2179), tensor(-7.0695), tensor(-120.5180), tensor(-46.2490), tensor(-67.1543), tensor(-19.2786), tensor(-14.7851), tensor(-1.2830), tensor(-16.8054), tensor(-59.7536), tensor(-10.3751), tensor(-48.4648), tensor(-42.0798), tensor(-6.1236), tensor(-11.2406), tensor(-21.0423), tensor(-14.9290), tensor(-2.4025), tensor(-45.0803), tensor(-2.5345), tensor(-7.6932), tensor(-4.8695), tensor(-2.1148), tensor(-58.8646), tensor(-3.5196), tensor(-107.5736), tensor(-46.0237), tensor(-5.1502), tensor(-28.5728), tensor(-117.3545), tensor(-22.4782), tensor(-30.0780), tensor(-92.6685), tensor(-102.2199), tensor(-6.1676), tensor(-74.4898), tensor(-32.0777), tensor(-92.0043), tensor(-9.8186), tensor(-7.4883), tensor(-67.7116), tensor(-3.5350), tensor(-16.9757), tensor(-52.6485), tensor(-9.1869), tensor(-59.5417), tensor(-26.9310), tensor(-32.6642), tensor(-5.6413), tensor(-50.2168), tensor(-46.7029), tensor(-1.4953), tensor(-42.7018), tensor(-53.4075), tensor(-17.1461), tensor(-108.4873), tensor(-93.0602), tensor(-6.8421), tensor(-9.9727), tensor(-52.3680), tensor(-101.8621), tensor(-73.1349), tensor(-67.4479), tensor(-5.5192), tensor(-19.9916), tensor(-98.6287), tensor(-72.0161), tensor(-18.7535), tensor(-26.3320), tensor(-46.7415), tensor(-112.8999), tensor(-27.5808), tensor(-3.5697), tensor(-13.9250)], 'recall': [tensor(0.0451), tensor(0.0017), tensor(0.2444), tensor(0.0087), tensor(0.1146), tensor(0.0139), tensor(0.0052), tensor(2.7793e-05), tensor(0.0035), tensor(0.1094), tensor(2.4464e-05), tensor(0.0486), tensor(0.), tensor(0.0052), tensor(7.4012e-05), tensor(0.0122), tensor(7.6481e-06), tensor(0.0226), tensor(0.), tensor(0.), tensor(0.2865), tensor(0.7378), tensor(0.2170), tensor(0.0069), tensor(0.0417), tensor(0.0886), tensor(0.0868), tensor(0.), tensor(0.0642), tensor(0.0019), tensor(0.0106), tensor(0.1076), tensor(0.), tensor(4.9694e-06), tensor(0.0573), tensor(6.4652e-07), tensor(6.8058e-06), tensor(5.3236e-06), tensor(0.1719), tensor(0.3385), tensor(0.), tensor(1.2150e-05), tensor(3.6287e-05), tensor(0.), tensor(0.0799), tensor(0.), tensor(0.1545), tensor(0.), tensor(0.0004), tensor(0.), tensor(1.9372e-05), tensor(0.), tensor(0.0070), tensor(0.), tensor(0.0002), tensor(0.0194), tensor(0.0260), tensor(0.0139), tensor(0.0521), tensor(0.0453), tensor(0.0035), tensor(0.0001), tensor(0.), tensor(0.0330), tensor(0.), tensor(0.0139), tensor(0.0035), tensor(0.1267), tensor(1.9575e-07), tensor(0.1233), tensor(2.5505e-05), tensor(0.), tensor(0.), tensor(1.3842e-07), tensor(1.0651e-05), tensor(0.0296), tensor(0.0486), tensor(0.0069), tensor(0.0295), tensor(0.0208), tensor(0.), tensor(0.1528), tensor(0.3038), tensor(0.), tensor(0.0156), tensor(0.0747), tensor(0.0799), tensor(1.1239e-05), tensor(0.), tensor(0.0469), tensor(0.0418), tensor(0.0139), tensor(0.0139), tensor(0.0191), tensor(0.), tensor(0.2431), tensor(0.), tensor(0.), tensor(0.0017), tensor(0.), tensor(0.3203), tensor(0.0001), tensor(0.), tensor(0.0035), tensor(0.5677), tensor(1.4363e-05), tensor(0.), tensor(0.0799), tensor(1.3147e-05), tensor(0.0764), tensor(4.0259e-05), tensor(0.2101), tensor(0.1893), tensor(0.4132), tensor(0.0590), tensor(0.0417), tensor(0.4462), tensor(5.5244e-05), tensor(6.9828e-06), tensor(0.0312), tensor(0.1425), tensor(0.), tensor(0.0001), tensor(0.4410), tensor(0.), tensor(0.3993), tensor(0.1354), tensor(0.1199), tensor(0.1076), tensor(9.1637e-06), tensor(0.1910), tensor(2.8690e-07), tensor(2.9328e-05), tensor(0.0938), tensor(6.8546e-05), tensor(0.), tensor(0.0469), tensor(0.), tensor(0.), tensor(0.), tensor(0.0417), tensor(0.), tensor(0.1632), tensor(0.0104), tensor(0.), tensor(0.1010), tensor(0.), tensor(0.1736), tensor(0.1667), tensor(0.0990), tensor(0.6441), tensor(0.), tensor(0.0007), tensor(0.), tensor(0.0052), tensor(0.0087), tensor(0.0521), tensor(0.7638), tensor(0.0052), tensor(0.0556), tensor(0.0156), tensor(3.1837e-05), tensor(0.), tensor(0.0125), tensor(0.0156), tensor(6.3166e-06), tensor(7.5181e-06), tensor(0.0139), tensor(0.0191), tensor(0.3681), tensor(0.0053), tensor(0.), tensor(0.), tensor(0.0208), tensor(0.), tensor(0.), tensor(7.3296e-07), tensor(0.0218), tensor(0.2361), tensor(0.0174)]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iou': 6.4742766e-05, 'f1': 0.00012909684, 'accuracy': -39.24348, 'recall': 0.06584109}\n"
     ]
    }
   ],
   "source": [
    "print({key: np.mean(value) for key, value in metrics_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,  # model output channels (number of classes in your dataset)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = (output > 0.5).float()\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = (output_baseline > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/inria-example-image.jpeg\", np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/inria-example-mask.jpeg\", np.array(msk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/inria-example-unet-plus-plus-output.jpeg\", np.array(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.),\n",
       " 'f1': tensor(0.),\n",
       " 'accuracy': tensor(-36.6243),\n",
       " 'recall': tensor(0.)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLabV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = (output > 0.5).float()\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = (output_baseline > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/inria-example-deep-lab-output.jpeg\", np.array(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.),\n",
       " 'f1': tensor(0.),\n",
       " 'accuracy': tensor(-20.1664),\n",
       " 'recall': tensor(0.2431)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLabV3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = (output > 0.5).float()\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = (output_baseline > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/inria-example-deep-lab-v3-plus-output.jpeg\", np.array(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.),\n",
       " 'f1': tensor(0.),\n",
       " 'accuracy': tensor(-25.4879),\n",
       " 'recall': tensor(0.)}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
