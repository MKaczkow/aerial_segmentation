{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from src.models.BaselineModel import BaselineModel\n",
    "from src.evaluation.evaluate_result import evaluate_result\n",
    "from src.datasets.INRIAAerialImageLabellingDataset import (\n",
    "    INRIAAerialImageLabellingDataset,\n",
    ")\n",
    "\n",
    "from src.datasets.utils.ResizeToDivisibleBy32 import ResizeToDivisibleBy32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "BATCH_SIZE = 1\n",
    "SEED = 42\n",
    "INRIA_DATASET_PATH = \"data/INRIAAerialImageLabellingDataset\"\n",
    "IMAGE_SIZE = 576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/INRIAAerialImageLabellingDataset\\train\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "labeled_dataset = INRIAAerialImageLabellingDataset(\n",
    "    INRIA_DATASET_PATH, transforms=[\n",
    "        Resize(IMAGE_SIZE),\n",
    "        # ResizeToDivisibleBy32()\n",
    "        ]\n",
    ")\n",
    "print(len(labeled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/INRIAAerialImageLabellingDataset\\test\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "test_dataset = INRIAAerialImageLabellingDataset(\n",
    "    INRIA_DATASET_PATH, split=\"test\", transforms=[\n",
    "        Resize(IMAGE_SIZE),\n",
    "        # ResizeToDivisibleBy32()\n",
    "        ]\n",
    ")\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,  # model output channels (number of classes in your dataset)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 576])\n",
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = (output > 0.5).float()\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = (output_baseline > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 576, 576])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 25126332, 255: 114244}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(outp, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 25126332, 1.0: 114244}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(output.cpu(), return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_outp = to_pil_transform(output_baseline.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 12622155, 255: 12618421}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(rand_outp, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.0053),\n",
       " 'f1': tensor(0.0105),\n",
       " 'accuracy': tensor(0.6975),\n",
       " 'recall': tensor(0.0057)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.2314),\n",
       " 'f1': tensor(0.3759),\n",
       " 'accuracy': tensor(0.5000),\n",
       " 'recall': tensor(0.4998)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output_baseline, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(1.),\n",
       " 'f1': tensor(1.),\n",
       " 'accuracy': tensor(1.),\n",
       " 'recall': tensor(1.)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(masks, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "metrics_dict = {\n",
    "    \"iou\": [],\n",
    "    \"f1\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        output = model(images.to(device))\n",
    "        output = (output > 0.5).float()\n",
    "\n",
    "        iter_metrics = evaluate_result(output, masks)\n",
    "        for key in metrics_dict.keys():\n",
    "            metrics_dict[key].append(iter_metrics[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iou': [tensor(0.0257), tensor(0.0089), tensor(0.0280), tensor(0.0279), tensor(0.0046), tensor(0.0053), tensor(0.0109), tensor(0.0063), tensor(0.0075), tensor(0.0046), tensor(0.0026), tensor(0.0040), tensor(0.0050), tensor(0.0666), tensor(0.0043), tensor(0.0323), tensor(0.0087), tensor(0.0068), tensor(0.0095), tensor(0.0042), tensor(0.0197), tensor(0.0040), tensor(0.0016), tensor(0.0178), tensor(0.0068), tensor(0.0022), tensor(0.0163), tensor(0.0114), tensor(0.0041), tensor(0.0232), tensor(0.0023), tensor(0.0026), tensor(0.0089), tensor(0.0044), tensor(0.0100), tensor(0.0050), tensor(0.0188), tensor(0.0029), tensor(0.0039), tensor(0.0066), tensor(0.0156), tensor(0.0092), tensor(0.0075), tensor(0.0040), tensor(0.0024), tensor(0.0072), tensor(0.0022), tensor(0.0067), tensor(0.0036), tensor(0.0048), tensor(0.0084), tensor(0.0106), tensor(0.0479), tensor(0.0037), tensor(0.0019), tensor(0.0094), tensor(0.0253), tensor(0.0056), tensor(0.0021), tensor(0.0021), tensor(0.0051), tensor(0.0089), tensor(0.0041), tensor(0.0066), tensor(0.0027), tensor(0.0033), tensor(0.0247), tensor(0.0134), tensor(0.0042), tensor(0.0061), tensor(0.0028), tensor(0.0023), tensor(0.0044), tensor(0.0070), tensor(0.0102), tensor(0.0038), tensor(0.0044), tensor(0.0092), tensor(0.0130), tensor(0.0086), tensor(0.0084), tensor(0.0056), tensor(0.0093), tensor(0.0037), tensor(0.0077), tensor(0.0818), tensor(0.0041), tensor(0.0036), tensor(0.0557), tensor(0.0033), tensor(0.0095), tensor(0.0068), tensor(0.0277), tensor(0.0044), tensor(0.0019), tensor(0.0035), tensor(0.0063), tensor(0.0053), tensor(0.0408), tensor(0.0048), tensor(0.0057), tensor(0.0037), tensor(0.0125), tensor(0.0074), tensor(0.0135), tensor(0.0047), tensor(0.0042), tensor(0.0050), tensor(0.0106), tensor(0.0026), tensor(0.0473), tensor(0.0022), tensor(0.0043), tensor(0.0025), tensor(0.0050), tensor(0.0236), tensor(0.0037), tensor(0.0048), tensor(0.0056), tensor(0.0149), tensor(0.0064), tensor(0.0032), tensor(0.0095), tensor(0.0138), tensor(0.0043), tensor(0.0045), tensor(0.0055), tensor(0.0101), tensor(0.0052), tensor(0.0171), tensor(0.0073), tensor(0.0078), tensor(0.0075), tensor(0.0020), tensor(0.0074), tensor(0.0039), tensor(0.0375), tensor(0.0053), tensor(0.0065), tensor(0.0159), tensor(0.0430), tensor(0.0149), tensor(0.0047), tensor(0.0042), tensor(0.0067), tensor(0.0023), tensor(0.0626), tensor(0.0082), tensor(0.0023), tensor(0.0044), tensor(0.0039), tensor(0.0114), tensor(0.0499), tensor(0.0120), tensor(0.0045), tensor(0.0087), tensor(0.0070), tensor(0.0048), tensor(0.0020), tensor(0.0033), tensor(0.0072), tensor(0.0118), tensor(0.0027), tensor(0.0031), tensor(0.0025), tensor(0.0045), tensor(0.0224), tensor(0.0136), tensor(0.0540), tensor(0.0047), tensor(0.0063), tensor(0.0458), tensor(0.0031), tensor(0.0095), tensor(0.0116), tensor(0.0040), tensor(0.0035), tensor(0.0004), tensor(0.0062), tensor(0.0031)], 'f1': [tensor(0.0501), tensor(0.0176), tensor(0.0544), tensor(0.0543), tensor(0.0092), tensor(0.0105), tensor(0.0215), tensor(0.0126), tensor(0.0150), tensor(0.0092), tensor(0.0052), tensor(0.0080), tensor(0.0100), tensor(0.1248), tensor(0.0086), tensor(0.0626), tensor(0.0172), tensor(0.0135), tensor(0.0188), tensor(0.0083), tensor(0.0387), tensor(0.0079), tensor(0.0032), tensor(0.0350), tensor(0.0135), tensor(0.0045), tensor(0.0322), tensor(0.0226), tensor(0.0081), tensor(0.0454), tensor(0.0045), tensor(0.0053), tensor(0.0177), tensor(0.0087), tensor(0.0198), tensor(0.0099), tensor(0.0369), tensor(0.0057), tensor(0.0077), tensor(0.0131), tensor(0.0308), tensor(0.0182), tensor(0.0148), tensor(0.0080), tensor(0.0048), tensor(0.0142), tensor(0.0043), tensor(0.0132), tensor(0.0071), tensor(0.0096), tensor(0.0167), tensor(0.0209), tensor(0.0914), tensor(0.0074), tensor(0.0039), tensor(0.0186), tensor(0.0494), tensor(0.0111), tensor(0.0041), tensor(0.0043), tensor(0.0101), tensor(0.0176), tensor(0.0081), tensor(0.0131), tensor(0.0055), tensor(0.0066), tensor(0.0481), tensor(0.0264), tensor(0.0085), tensor(0.0122), tensor(0.0056), tensor(0.0045), tensor(0.0087), tensor(0.0139), tensor(0.0201), tensor(0.0075), tensor(0.0088), tensor(0.0183), tensor(0.0258), tensor(0.0170), tensor(0.0167), tensor(0.0111), tensor(0.0185), tensor(0.0073), tensor(0.0153), tensor(0.1513), tensor(0.0081), tensor(0.0072), tensor(0.1055), tensor(0.0065), tensor(0.0189), tensor(0.0136), tensor(0.0538), tensor(0.0087), tensor(0.0037), tensor(0.0070), tensor(0.0126), tensor(0.0105), tensor(0.0784), tensor(0.0095), tensor(0.0114), tensor(0.0075), tensor(0.0248), tensor(0.0147), tensor(0.0266), tensor(0.0094), tensor(0.0083), tensor(0.0099), tensor(0.0210), tensor(0.0052), tensor(0.0902), tensor(0.0044), tensor(0.0086), tensor(0.0050), tensor(0.0099), tensor(0.0462), tensor(0.0073), tensor(0.0096), tensor(0.0112), tensor(0.0294), tensor(0.0128), tensor(0.0064), tensor(0.0188), tensor(0.0273), tensor(0.0085), tensor(0.0089), tensor(0.0110), tensor(0.0200), tensor(0.0103), tensor(0.0337), tensor(0.0145), tensor(0.0154), tensor(0.0150), tensor(0.0040), tensor(0.0147), tensor(0.0077), tensor(0.0723), tensor(0.0105), tensor(0.0128), tensor(0.0313), tensor(0.0825), tensor(0.0293), tensor(0.0094), tensor(0.0084), tensor(0.0133), tensor(0.0046), tensor(0.1179), tensor(0.0162), tensor(0.0046), tensor(0.0087), tensor(0.0078), tensor(0.0226), tensor(0.0951), tensor(0.0237), tensor(0.0090), tensor(0.0173), tensor(0.0138), tensor(0.0095), tensor(0.0041), tensor(0.0066), tensor(0.0144), tensor(0.0234), tensor(0.0054), tensor(0.0062), tensor(0.0051), tensor(0.0090), tensor(0.0438), tensor(0.0268), tensor(0.1024), tensor(0.0094), tensor(0.0125), tensor(0.0876), tensor(0.0062), tensor(0.0188), tensor(0.0229), tensor(0.0079), tensor(0.0070), tensor(0.0008), tensor(0.0124), tensor(0.0063)], 'accuracy': [tensor(0.9890), tensor(0.6669), tensor(0.9951), tensor(0.9752), tensor(0.9272), tensor(0.9192), tensor(0.9402), tensor(0.7743), tensor(0.7141), tensor(0.8198), tensor(0.8906), tensor(0.6745), tensor(0.9057), tensor(0.9610), tensor(0.8356), tensor(0.9686), tensor(0.7781), tensor(0.8873), tensor(0.9645), tensor(0.5824), tensor(0.8920), tensor(0.6438), tensor(0.5394), tensor(0.9854), tensor(0.7678), tensor(0.8413), tensor(0.9674), tensor(0.9800), tensor(0.9079), tensor(0.9778), tensor(0.7136), tensor(0.6847), tensor(0.8379), tensor(0.5860), tensor(0.9266), tensor(0.8285), tensor(0.9701), tensor(0.6200), tensor(0.8808), tensor(0.9524), tensor(0.9665), tensor(0.8982), tensor(0.9321), tensor(0.7479), tensor(0.9113), tensor(0.7931), tensor(0.7376), tensor(0.8373), tensor(0.9591), tensor(0.7933), tensor(0.8838), tensor(0.7704), tensor(0.9724), tensor(0.8125), tensor(0.8283), tensor(0.8499), tensor(0.9804), tensor(0.7361), tensor(0.8713), tensor(0.5495), tensor(0.7245), tensor(0.9263), tensor(0.8145), tensor(0.9476), tensor(0.6315), tensor(0.6278), tensor(0.9414), tensor(0.9090), tensor(0.8013), tensor(0.9474), tensor(0.8183), tensor(0.8619), tensor(0.8758), tensor(0.8323), tensor(0.9579), tensor(0.7120), tensor(0.8466), tensor(0.9627), tensor(0.8792), tensor(0.9240), tensor(0.8937), tensor(0.7215), tensor(0.9470), tensor(0.8441), tensor(0.9379), tensor(0.9773), tensor(0.9131), tensor(0.6631), tensor(0.9697), tensor(0.7072), tensor(0.9388), tensor(0.9230), tensor(0.9556), tensor(0.8590), tensor(0.7066), tensor(0.7942), tensor(0.9128), tensor(0.6975), tensor(0.9702), tensor(0.8316), tensor(0.8270), tensor(0.8276), tensor(0.9187), tensor(0.7916), tensor(0.6020), tensor(0.7089), tensor(0.8700), tensor(0.8875), tensor(0.9345), tensor(0.7720), tensor(0.9672), tensor(0.7665), tensor(0.9778), tensor(0.8828), tensor(0.7326), tensor(0.9132), tensor(0.8703), tensor(0.7992), tensor(0.7376), tensor(0.9515), tensor(0.7983), tensor(0.7816), tensor(0.9485), tensor(0.9584), tensor(0.8249), tensor(0.8470), tensor(0.9227), tensor(0.9271), tensor(0.7512), tensor(0.9530), tensor(0.8150), tensor(0.9162), tensor(0.9263), tensor(0.8610), tensor(0.7338), tensor(0.8312), tensor(0.9842), tensor(0.9794), tensor(0.8268), tensor(0.9719), tensor(0.9703), tensor(0.9573), tensor(0.7813), tensor(0.7830), tensor(0.8795), tensor(0.6900), tensor(0.9866), tensor(0.8224), tensor(0.5249), tensor(0.9098), tensor(0.7791), tensor(0.9405), tensor(0.9879), tensor(0.9165), tensor(0.7575), tensor(0.9698), tensor(0.7700), tensor(0.6420), tensor(0.7738), tensor(0.8063), tensor(0.7891), tensor(0.9648), tensor(0.6092), tensor(0.7867), tensor(0.5671), tensor(0.6408), tensor(0.9692), tensor(0.9315), tensor(0.9770), tensor(0.8217), tensor(0.8196), tensor(0.9657), tensor(0.9054), tensor(0.8564), tensor(0.7645), tensor(0.8047), tensor(0.8688), tensor(0.8438), tensor(0.8504), tensor(0.7657)], 'recall': [tensor(0.7829), tensor(0.0418), tensor(0.7908), tensor(0.3123), tensor(0.0109), tensor(0.0227), tensor(0.2860), tensor(0.0064), tensor(0.0392), tensor(0.0047), tensor(0.0308), tensor(0.0043), tensor(0.0283), tensor(0.1441), tensor(0.0042), tensor(0.1133), tensor(0.1153), tensor(0.0083), tensor(0.3004), tensor(0.0043), tensor(0.0246), tensor(0.0039), tensor(0.0016), tensor(0.5034), tensor(0.0172), tensor(0.0024), tensor(0.0795), tensor(0.6321), tensor(0.1000), tensor(0.2705), tensor(0.0122), tensor(0.0106), tensor(0.0335), tensor(0.0045), tensor(0.0978), tensor(0.0056), tensor(0.1591), tensor(0.0030), tensor(0.0041), tensor(0.4688), tensor(0.2258), tensor(0.0127), tensor(0.0145), tensor(0.0640), tensor(0.3753), tensor(0.0076), tensor(0.0204), tensor(0.0073), tensor(0.6626), tensor(0.1239), tensor(0.0077), tensor(0.0448), tensor(0.2109), tensor(0.0035), tensor(0.0026), tensor(0.0106), tensor(0.2553), tensor(0.0061), tensor(0.1825), tensor(0.0021), tensor(0.0060), tensor(0.2117), tensor(0.0045), tensor(0.0538), tensor(0.0030), tensor(0.0033), tensor(0.0262), tensor(0.0896), tensor(0.0380), tensor(0.2160), tensor(0.0030), tensor(0.0023), tensor(0.0430), tensor(0.0200), tensor(0.1833), tensor(0.0044), tensor(0.0046), tensor(0.1849), tensor(0.0144), tensor(0.0087), tensor(0.0096), tensor(0.0056), tensor(0.2096), tensor(0.0038), tensor(0.0090), tensor(0.2338), tensor(0.2878), tensor(0.0035), tensor(0.1044), tensor(0.0041), tensor(0.0432), tensor(0.3825), tensor(0.0679), tensor(0.0044), tensor(0.0252), tensor(0.0035), tensor(0.0322), tensor(0.0057), tensor(0.1299), tensor(0.0047), tensor(0.0058), tensor(0.0945), tensor(0.0298), tensor(0.0779), tensor(0.0207), tensor(0.0053), tensor(0.0044), tensor(0.0051), tensor(0.0701), tensor(0.0026), tensor(0.1481), tensor(0.0027), tensor(0.2744), tensor(0.0246), tensor(0.0068), tensor(0.0251), tensor(0.0039), tensor(0.0172), tensor(0.0070), tensor(0.0968), tensor(0.0068), tensor(0.0096), tensor(0.3187), tensor(0.1824), tensor(0.0041), tensor(0.0045), tensor(0.0429), tensor(0.0328), tensor(0.0113), tensor(0.0178), tensor(0.0849), tensor(0.2450), tensor(0.1337), tensor(0.0132), tensor(0.0399), tensor(0.0194), tensor(0.2523), tensor(0.4443), tensor(0.0067), tensor(0.3189), tensor(0.1898), tensor(0.1608), tensor(0.0111), tensor(0.0055), tensor(0.0068), tensor(0.0195), tensor(0.3156), tensor(0.0084), tensor(0.0023), tensor(0.0167), tensor(0.0520), tensor(0.0448), tensor(0.5388), tensor(0.1334), tensor(0.0383), tensor(0.3855), tensor(0.0076), tensor(0.0324), tensor(0.0462), tensor(0.0034), tensor(0.0230), tensor(0.1453), tensor(0.0029), tensor(0.0033), tensor(0.0026), tensor(0.0054), tensor(0.0790), tensor(0.2545), tensor(0.1530), tensor(0.1067), tensor(0.0064), tensor(0.0941), tensor(0.2336), tensor(0.0886), tensor(0.1141), tensor(0.0302), tensor(0.0034), tensor(0.0419), tensor(0.0069), tensor(0.0659)]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iou': 0.010615132, 'f1': 0.020689072, 'accuracy': 0.8447621, 'recall': 0.0944909}\n"
     ]
    }
   ],
   "source": [
    "print({key: np.mean(value) for key, value in metrics_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,  # model output channels (number of classes in your dataset)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 5024, 5024])\n",
      "torch.Size([1, 1, 5024, 5024])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.52 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 16.28 GiB is allocated by PyTorch, and 7.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(masks\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m (output \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     10\u001b[0m output_baseline \u001b[38;5;241m=\u001b[39m baseline_model(images\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:30\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m     29\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 30\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\segmentation_models_pytorch\\decoders\\unetplusplus\\decoder.py:135\u001b[0m, in \u001b[0;36mUnetPlusPlusDecoder.forward\u001b[1;34m(self, *features)\u001b[0m\n\u001b[0;32m    133\u001b[0m             cat_features \u001b[38;5;241m=\u001b[39m [dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_l_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dense_l_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    134\u001b[0m             cat_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(cat_features \u001b[38;5;241m+\u001b[39m [features[dense_l_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m             dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_l_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdense_l_i\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdense_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdepth_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdense_l_i\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m](dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dense_x[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\segmentation_models_pytorch\\decoders\\unetplusplus\\decoder.py:38\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x, skip)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1(x)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.52 GiB. GPU 0 has a total capacity of 12.00 GiB of which 0 bytes is free. Of the allocated memory 16.28 GiB is allocated by PyTorch, and 7.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = (output > 0.5).float()\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = (output_baseline > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
