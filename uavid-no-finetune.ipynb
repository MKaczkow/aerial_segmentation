{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, Compose\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from src.models.BaselineModel import BaselineModel\n",
    "from src.evaluation.evaluate_result import evaluate_result\n",
    "from src.datasets.UAVidSemanticSegmentationDataset import (\n",
    "    UAVidSemanticSegmentationDataset,\n",
    ")\n",
    "\n",
    "from src.datasets.utils.ResizeToDivisibleBy32 import ResizeToDivisibleBy32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SIZE = 0.2\n",
    "BATCH_SIZE = 1\n",
    "SEED = 42\n",
    "UAVID_DATASET_PATH = \"data/UAVidSemanticSegmentationDataset\"\n",
    "IMAGE_SIZE = 576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "train_dataset = UAVidSemanticSegmentationDataset(\n",
    "    UAVID_DATASET_PATH,\n",
    "    transforms=[Compose([Resize(IMAGE_SIZE), ResizeToDivisibleBy32()])],\n",
    ")\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "val_dataset = UAVidSemanticSegmentationDataset(\n",
    "    UAVID_DATASET_PATH,\n",
    "    split=\"valid\",\n",
    "    transforms=[Compose([Resize(IMAGE_SIZE), ResizeToDivisibleBy32()])],\n",
    ")\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "test_dataset = UAVidSemanticSegmentationDataset(\n",
    "    UAVID_DATASET_PATH,\n",
    "    split=\"test\",\n",
    "    transforms=[Compose([Resize(IMAGE_SIZE), ResizeToDivisibleBy32()])],\n",
    ")\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1024])\n",
      "torch.Size([1, 1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1024])\n",
      "torch.Size([1, 1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in val_loader:\n",
    "    print(images.shape)\n",
    "    print(masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "for images in test_loader:\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=8,  # model output channels (number of classes in your dataset)\n",
    "    activation=\"softmax\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel(classes=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1024])\n",
      "torch.Size([1, 1, 576, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\__repos\\aerial_segmentation\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = torch.argmax(output, dim=1)\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = torch.argmax(output_baseline, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 421156, 1: 171, 2: 1, 3: 41, 4: 64813, 5: 94888, 6: 5675, 7: 3079}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(output.cpu(), return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.0950),\n",
       " 'f1': tensor(0.1735),\n",
       " 'accuracy': tensor(0.7934),\n",
       " 'recall': tensor(0.1735)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks, mode=\"multiclass\", num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.0667),\n",
       " 'f1': tensor(0.1251),\n",
       " 'accuracy': tensor(0.7813),\n",
       " 'recall': tensor(0.1251)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output_baseline, masks, mode=\"multiclass\", num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "metrics_dict = {\n",
    "    \"iou\": [],\n",
    "    \"f1\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"recall\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        output = model(images.to(device))\n",
    "        output = torch.argmax(output, dim=1)\n",
    "\n",
    "\n",
    "        iter_metrics = evaluate_result(output, masks, mode=\"multiclass\", num_classes=8)\n",
    "        for key in metrics_dict.keys():\n",
    "            metrics_dict[key].append(iter_metrics[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iou': [tensor(0.1571), tensor(0.0709), tensor(0.1364), tensor(0.1650), tensor(0.2344), tensor(0.1098), tensor(0.1290), tensor(0.1248), tensor(0.0845), tensor(0.0191), tensor(0.1105), tensor(0.0675), tensor(0.0961), tensor(0.0961), tensor(0.0704), tensor(0.0794), tensor(0.1482), tensor(0.1021), tensor(0.0582), tensor(0.0715), tensor(0.0797), tensor(0.0438), tensor(0.1321), tensor(0.1098), tensor(0.0524), tensor(0.1219), tensor(0.0703), tensor(0.1151), tensor(0.0544), tensor(0.0735), tensor(0.0958), tensor(0.0929), tensor(0.0289), tensor(0.0843), tensor(0.1763), tensor(0.0966), tensor(0.0767), tensor(0.0891), tensor(0.0537), tensor(0.1011), tensor(0.1655), tensor(0.1204), tensor(0.0713), tensor(0.1453), tensor(0.0658), tensor(0.0804), tensor(0.0494), tensor(0.0867), tensor(0.0377), tensor(0.1068), tensor(0.0613), tensor(0.1534), tensor(0.1060), tensor(0.1174), tensor(0.0909), tensor(0.0256), tensor(0.0819), tensor(0.1090), tensor(0.0652), tensor(0.0334), tensor(0.0724), tensor(0.1173), tensor(0.1050), tensor(0.0950), tensor(0.0498), tensor(0.1571), tensor(0.1046), tensor(0.0990), tensor(0.0833), tensor(0.0614), tensor(0.0547), tensor(0.0789), tensor(0.0670), tensor(0.0166), tensor(0.0821), tensor(0.0369), tensor(0.0179), tensor(0.0698), tensor(0.0609), tensor(0.0527), tensor(0.0794), tensor(0.0791), tensor(0.0171), tensor(0.0584), tensor(0.0340), tensor(0.1463), tensor(0.1163), tensor(0.1537), tensor(0.0846), tensor(0.0661), tensor(0.1482), tensor(0.1070), tensor(0.1427), tensor(0.0308), tensor(0.0492), tensor(0.1020), tensor(0.0867), tensor(0.0981), tensor(0.0919), tensor(0.0317), tensor(0.0846), tensor(0.1815), tensor(0.0896), tensor(0.0796), tensor(0.1293), tensor(0.1105), tensor(0.0783), tensor(0.0910), tensor(0.0756), tensor(0.0715), tensor(0.0760), tensor(0.0300), tensor(0.1089), tensor(0.1166), tensor(0.0922), tensor(0.1147), tensor(0.0207), tensor(0.0735), tensor(0.0849), tensor(0.0141), tensor(0.1723), tensor(0.0319), tensor(0.0524), tensor(0.0958), tensor(0.1606), tensor(0.0464), tensor(0.1009), tensor(0.1129), tensor(0.0709), tensor(0.0505), tensor(0.0706), tensor(0.1147), tensor(0.0873), tensor(0.1397), tensor(0.1223), tensor(0.0717), tensor(0.1275), tensor(0.1604), tensor(0.0907), tensor(0.0791), tensor(0.0699), tensor(0.1354), tensor(0.0786), tensor(0.1093), tensor(0.0732), tensor(0.0785), tensor(0.0914), tensor(0.0281), tensor(0.0462), tensor(0.1021), tensor(0.1117), tensor(0.0345), tensor(0.0971), tensor(0.0250), tensor(0.0710), tensor(0.0923), tensor(0.1420), tensor(0.0874), tensor(0.0814), tensor(0.0817), tensor(0.0546), tensor(0.0945), tensor(0.0176), tensor(0.0860), tensor(0.0855), tensor(0.0771), tensor(0.1267), tensor(0.0456), tensor(0.1963), tensor(0.1000), tensor(0.0472), tensor(0.1088), tensor(0.0452), tensor(0.0654), tensor(0.0908), tensor(0.1245), tensor(0.0884), tensor(0.0839), tensor(0.0904), tensor(0.0989), tensor(0.0989), tensor(0.1219), tensor(0.0950), tensor(0.0651), tensor(0.0184), tensor(0.1325), tensor(0.0715), tensor(0.0857), tensor(0.1371), tensor(0.1611), tensor(0.1219), tensor(0.1477), tensor(0.0960), tensor(0.0882), tensor(0.0571), tensor(0.0976), tensor(0.0642), tensor(0.0703), tensor(0.0667), tensor(0.0926)], 'f1': [tensor(0.2715), tensor(0.1324), tensor(0.2401), tensor(0.2833), tensor(0.3798), tensor(0.1979), tensor(0.2285), tensor(0.2219), tensor(0.1558), tensor(0.0376), tensor(0.1990), tensor(0.1265), tensor(0.1753), tensor(0.1753), tensor(0.1315), tensor(0.1472), tensor(0.2581), tensor(0.1853), tensor(0.1099), tensor(0.1334), tensor(0.1476), tensor(0.0840), tensor(0.2333), tensor(0.1978), tensor(0.0996), tensor(0.2173), tensor(0.1313), tensor(0.2065), tensor(0.1031), tensor(0.1370), tensor(0.1748), tensor(0.1701), tensor(0.0562), tensor(0.1555), tensor(0.2997), tensor(0.1762), tensor(0.1425), tensor(0.1636), tensor(0.1020), tensor(0.1837), tensor(0.2840), tensor(0.2150), tensor(0.1331), tensor(0.2537), tensor(0.1235), tensor(0.1488), tensor(0.0941), tensor(0.1595), tensor(0.0727), tensor(0.1930), tensor(0.1155), tensor(0.2660), tensor(0.1917), tensor(0.2101), tensor(0.1666), tensor(0.0499), tensor(0.1514), tensor(0.1966), tensor(0.1224), tensor(0.0647), tensor(0.1351), tensor(0.2099), tensor(0.1900), tensor(0.1735), tensor(0.0949), tensor(0.2715), tensor(0.1893), tensor(0.1802), tensor(0.1537), tensor(0.1156), tensor(0.1037), tensor(0.1463), tensor(0.1255), tensor(0.0327), tensor(0.1518), tensor(0.0712), tensor(0.0351), tensor(0.1304), tensor(0.1148), tensor(0.1001), tensor(0.1470), tensor(0.1466), tensor(0.0336), tensor(0.1104), tensor(0.0658), tensor(0.2553), tensor(0.2084), tensor(0.2664), tensor(0.1561), tensor(0.1239), tensor(0.2582), tensor(0.1934), tensor(0.2497), tensor(0.0597), tensor(0.0938), tensor(0.1851), tensor(0.1595), tensor(0.1787), tensor(0.1684), tensor(0.0615), tensor(0.1560), tensor(0.3073), tensor(0.1644), tensor(0.1474), tensor(0.2290), tensor(0.1989), tensor(0.1452), tensor(0.1669), tensor(0.1405), tensor(0.1334), tensor(0.1412), tensor(0.0583), tensor(0.1963), tensor(0.2089), tensor(0.1688), tensor(0.2059), tensor(0.0406), tensor(0.1370), tensor(0.1564), tensor(0.0278), tensor(0.2940), tensor(0.0619), tensor(0.0995), tensor(0.1749), tensor(0.2767), tensor(0.0887), tensor(0.1833), tensor(0.2029), tensor(0.1324), tensor(0.0962), tensor(0.1319), tensor(0.2058), tensor(0.1606), tensor(0.2452), tensor(0.2180), tensor(0.1339), tensor(0.2262), tensor(0.2764), tensor(0.1662), tensor(0.1466), tensor(0.1307), tensor(0.2386), tensor(0.1457), tensor(0.1971), tensor(0.1365), tensor(0.1455), tensor(0.1675), tensor(0.0547), tensor(0.0883), tensor(0.1853), tensor(0.2010), tensor(0.0666), tensor(0.1769), tensor(0.0488), tensor(0.1326), tensor(0.1689), tensor(0.2486), tensor(0.1608), tensor(0.1505), tensor(0.1510), tensor(0.1035), tensor(0.1726), tensor(0.0345), tensor(0.1583), tensor(0.1576), tensor(0.1432), tensor(0.2249), tensor(0.0873), tensor(0.3282), tensor(0.1818), tensor(0.0901), tensor(0.1962), tensor(0.0866), tensor(0.1228), tensor(0.1665), tensor(0.2214), tensor(0.1624), tensor(0.1547), tensor(0.1658), tensor(0.1800), tensor(0.1799), tensor(0.2173), tensor(0.1735), tensor(0.1222), tensor(0.0362), tensor(0.2339), tensor(0.1335), tensor(0.1579), tensor(0.2412), tensor(0.2775), tensor(0.2172), tensor(0.2573), tensor(0.1752), tensor(0.1622), tensor(0.1081), tensor(0.1778), tensor(0.1207), tensor(0.1315), tensor(0.1251), tensor(0.1696)], 'accuracy': [tensor(0.8179), tensor(0.7831), tensor(0.8100), tensor(0.8208), tensor(0.8449), tensor(0.7995), tensor(0.8071), tensor(0.8055), tensor(0.7890), tensor(0.7594), tensor(0.7998), tensor(0.7816), tensor(0.7938), tensor(0.7938), tensor(0.7829), tensor(0.7868), tensor(0.8145), tensor(0.7963), tensor(0.7775), tensor(0.7833), tensor(0.7869), tensor(0.7710), tensor(0.8083), tensor(0.7995), tensor(0.7749), tensor(0.8043), tensor(0.7828), tensor(0.8016), tensor(0.7758), tensor(0.7843), tensor(0.7937), tensor(0.7925), tensor(0.7641), tensor(0.7889), tensor(0.8249), tensor(0.7941), tensor(0.7856), tensor(0.7909), tensor(0.7755), tensor(0.7959), tensor(0.8210), tensor(0.8037), tensor(0.7833), tensor(0.8134), tensor(0.7809), tensor(0.7872), tensor(0.7735), tensor(0.7899), tensor(0.7682), tensor(0.7983), tensor(0.7789), tensor(0.8165), tensor(0.7979), tensor(0.8025), tensor(0.7916), tensor(0.7625), tensor(0.7878), tensor(0.7991), tensor(0.7806), tensor(0.7662), tensor(0.7838), tensor(0.8025), tensor(0.7975), tensor(0.7934), tensor(0.7737), tensor(0.8179), tensor(0.7973), tensor(0.7950), tensor(0.7884), tensor(0.7789), tensor(0.7759), tensor(0.7866), tensor(0.7814), tensor(0.7582), tensor(0.7880), tensor(0.7678), tensor(0.7588), tensor(0.7826), tensor(0.7787), tensor(0.7750), tensor(0.7868), tensor(0.7866), tensor(0.7584), tensor(0.7776), tensor(0.7664), tensor(0.8138), tensor(0.8021), tensor(0.8166), tensor(0.7890), tensor(0.7810), tensor(0.8145), tensor(0.7983), tensor(0.8124), tensor(0.7649), tensor(0.7734), tensor(0.7963), tensor(0.7899), tensor(0.7947), tensor(0.7921), tensor(0.7654), tensor(0.7890), tensor(0.8268), tensor(0.7911), tensor(0.7869), tensor(0.8073), tensor(0.7997), tensor(0.7863), tensor(0.7917), tensor(0.7851), tensor(0.7833), tensor(0.7853), tensor(0.7646), tensor(0.7991), tensor(0.8022), tensor(0.7922), tensor(0.8015), tensor(0.7602), tensor(0.7843), tensor(0.7891), tensor(0.7570), tensor(0.8235), tensor(0.7655), tensor(0.7749), tensor(0.7937), tensor(0.8192), tensor(0.7722), tensor(0.7958), tensor(0.8007), tensor(0.7831), tensor(0.7740), tensor(0.7830), tensor(0.8015), tensor(0.7902), tensor(0.8113), tensor(0.8045), tensor(0.7835), tensor(0.8065), tensor(0.8191), tensor(0.7916), tensor(0.7867), tensor(0.7827), tensor(0.8096), tensor(0.7864), tensor(0.7993), tensor(0.7841), tensor(0.7864), tensor(0.7919), tensor(0.7637), tensor(0.7721), tensor(0.7963), tensor(0.8003), tensor(0.7667), tensor(0.7942), tensor(0.7622), tensor(0.7832), tensor(0.7922), tensor(0.8122), tensor(0.7902), tensor(0.7876), tensor(0.7878), tensor(0.7759), tensor(0.7932), tensor(0.7586), tensor(0.7896), tensor(0.7894), tensor(0.7858), tensor(0.8062), tensor(0.7718), tensor(0.8320), tensor(0.7955), tensor(0.7725), tensor(0.7991), tensor(0.7716), tensor(0.7807), tensor(0.7916), tensor(0.8054), tensor(0.7906), tensor(0.7887), tensor(0.7914), tensor(0.7950), tensor(0.7950), tensor(0.8043), tensor(0.7934), tensor(0.7805), tensor(0.7590), tensor(0.8085), tensor(0.7834), tensor(0.7895), tensor(0.8103), tensor(0.8194), tensor(0.8043), tensor(0.8143), tensor(0.7938), tensor(0.7905), tensor(0.7770), tensor(0.7944), tensor(0.7802), tensor(0.7829), tensor(0.7813), tensor(0.7924)], 'recall': [tensor(0.2715), tensor(0.1324), tensor(0.2401), tensor(0.2833), tensor(0.3798), tensor(0.1979), tensor(0.2285), tensor(0.2219), tensor(0.1558), tensor(0.0376), tensor(0.1990), tensor(0.1265), tensor(0.1753), tensor(0.1753), tensor(0.1315), tensor(0.1472), tensor(0.2581), tensor(0.1853), tensor(0.1099), tensor(0.1334), tensor(0.1476), tensor(0.0840), tensor(0.2333), tensor(0.1978), tensor(0.0996), tensor(0.2173), tensor(0.1313), tensor(0.2065), tensor(0.1031), tensor(0.1370), tensor(0.1748), tensor(0.1701), tensor(0.0562), tensor(0.1555), tensor(0.2997), tensor(0.1762), tensor(0.1425), tensor(0.1636), tensor(0.1020), tensor(0.1837), tensor(0.2840), tensor(0.2150), tensor(0.1331), tensor(0.2537), tensor(0.1235), tensor(0.1488), tensor(0.0941), tensor(0.1595), tensor(0.0727), tensor(0.1930), tensor(0.1155), tensor(0.2660), tensor(0.1917), tensor(0.2101), tensor(0.1666), tensor(0.0499), tensor(0.1514), tensor(0.1966), tensor(0.1224), tensor(0.0647), tensor(0.1351), tensor(0.2099), tensor(0.1900), tensor(0.1735), tensor(0.0949), tensor(0.2715), tensor(0.1893), tensor(0.1802), tensor(0.1537), tensor(0.1156), tensor(0.1037), tensor(0.1463), tensor(0.1255), tensor(0.0327), tensor(0.1518), tensor(0.0712), tensor(0.0351), tensor(0.1304), tensor(0.1148), tensor(0.1001), tensor(0.1470), tensor(0.1466), tensor(0.0336), tensor(0.1104), tensor(0.0658), tensor(0.2553), tensor(0.2084), tensor(0.2664), tensor(0.1561), tensor(0.1239), tensor(0.2582), tensor(0.1934), tensor(0.2497), tensor(0.0597), tensor(0.0938), tensor(0.1851), tensor(0.1595), tensor(0.1787), tensor(0.1684), tensor(0.0615), tensor(0.1560), tensor(0.3073), tensor(0.1644), tensor(0.1474), tensor(0.2290), tensor(0.1989), tensor(0.1452), tensor(0.1669), tensor(0.1405), tensor(0.1334), tensor(0.1412), tensor(0.0583), tensor(0.1963), tensor(0.2089), tensor(0.1688), tensor(0.2059), tensor(0.0406), tensor(0.1370), tensor(0.1564), tensor(0.0278), tensor(0.2940), tensor(0.0619), tensor(0.0995), tensor(0.1749), tensor(0.2767), tensor(0.0887), tensor(0.1833), tensor(0.2029), tensor(0.1324), tensor(0.0962), tensor(0.1319), tensor(0.2058), tensor(0.1606), tensor(0.2452), tensor(0.2180), tensor(0.1339), tensor(0.2262), tensor(0.2764), tensor(0.1662), tensor(0.1466), tensor(0.1307), tensor(0.2386), tensor(0.1457), tensor(0.1971), tensor(0.1365), tensor(0.1455), tensor(0.1675), tensor(0.0547), tensor(0.0883), tensor(0.1853), tensor(0.2010), tensor(0.0666), tensor(0.1769), tensor(0.0488), tensor(0.1326), tensor(0.1689), tensor(0.2486), tensor(0.1608), tensor(0.1505), tensor(0.1510), tensor(0.1035), tensor(0.1726), tensor(0.0345), tensor(0.1583), tensor(0.1576), tensor(0.1432), tensor(0.2249), tensor(0.0873), tensor(0.3282), tensor(0.1818), tensor(0.0901), tensor(0.1962), tensor(0.0866), tensor(0.1228), tensor(0.1665), tensor(0.2214), tensor(0.1624), tensor(0.1547), tensor(0.1658), tensor(0.1800), tensor(0.1799), tensor(0.2173), tensor(0.1735), tensor(0.1222), tensor(0.0362), tensor(0.2339), tensor(0.1335), tensor(0.1579), tensor(0.2412), tensor(0.2775), tensor(0.2172), tensor(0.2573), tensor(0.1752), tensor(0.1622), tensor(0.1081), tensor(0.1778), tensor(0.1207), tensor(0.1315), tensor(0.1251), tensor(0.1696)]}\n"
     ]
    }
   ],
   "source": [
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=8,  # model output channels (number of classes in your dataset)\n",
    "    activation=\"softmax\",\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1120])\n",
      "torch.Size([1, 1, 576, 1120])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = torch.argmax(output, dim=1)\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = torch.argmax(output_baseline, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1120])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1120])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 50678, 1: 888, 2: 339, 4: 433271, 5: 640, 6: 89677, 7: 69627}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(output.to('cpu'), return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.int().squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imsave(\"assets/uavid-example-unet-plus-plus-output.jpeg\", np.array(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.1375),\n",
       " 'f1': tensor(0.2418),\n",
       " 'accuracy': tensor(0.8105),\n",
       " 'recall': tensor(0.2418)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks, mode=\"multiclass\", num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLabV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=8,\n",
    "    activation=\"softmax\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1120])\n",
      "torch.Size([1, 1, 576, 1120])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = torch.argmax(output, dim=1)\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = torch.argmax(output_baseline, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1120])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1120])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.int().squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/uavid-example-deep-lab-v3-output.jpeg\", np.array(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.1123),\n",
       " 'f1': tensor(0.2019),\n",
       " 'accuracy': tensor(0.8005),\n",
       " 'recall': tensor(0.2019)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks, mode=\"multiclass\", num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLabV3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run example model on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet18\",  # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",  # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=8,\n",
    "    activation=\"softmax\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 576, 1024])\n",
      "torch.Size([1, 1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in train_loader:\n",
    "        print(images.shape)\n",
    "        print(masks.shape)\n",
    "        break\n",
    "    output = model(images.to(device))\n",
    "    output = torch.argmax(output, dim=1)\n",
    "\n",
    "    output_baseline = baseline_model(images.to(device))\n",
    "    output_baseline = torch.argmax(output_baseline, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(output_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_transform = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = to_pil_transform(images.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = to_pil_transform(masks.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = to_pil_transform(output.int().squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave(\"assets/uavid-example-deep-lab-v3-plus-output.jpeg\", np.array(outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iou': tensor(0.0114),\n",
       " 'f1': tensor(0.0224),\n",
       " 'accuracy': tensor(0.7556),\n",
       " 'recall': tensor(0.0224)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result(output, masks, mode=\"multiclass\", num_classes=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
